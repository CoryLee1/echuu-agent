#!/usr/bin/env python3
"""
echuu - AI VTuber Performance Engine
=====================================
å•Agent + ä¸‰é˜¶æ®µæ¶æ„

ç”¨æˆ·è¾“å…¥: VTuberåç§°ã€è®¾å®šã€èƒŒæ™¯ã€ç›´æ’­ä¸»é¢˜
è¾“å‡º: é¢„ç½®è„šæœ¬ + å®æ—¶è¡¨æ¼”æ–‡æœ¬ï¼ˆå«å†…å¿ƒç‹¬ç™½ï¼‰

æ ¸å¿ƒåˆ›æ–°:
1. Interruption Cost - åŠ¨æ€å†³å®šæ˜¯å¦å›åº”å¼¹å¹•
2. Inner Monologueå¯è§ - è®©è§‚ä¼—çœ‹åˆ°AIçš„"æ€è€ƒè¿‡ç¨‹"  
3. æ•°æ®é©±åŠ¨ - ä»çœŸå®ä¸»æ’­åˆ‡ç‰‡å­¦ä¹ è¡Œä¸ºæ¨¡å¼
"""

import json
import os
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from enum import Enum
from collections import defaultdict, Counter
from openai import OpenAI

# ============================================================
# Part 1: æ•°æ®ç»“æ„
# ============================================================

class AttentionFocus(str, Enum):
    SELF = "self"           # è®²è‡ªå·±çš„äº‹
    AUDIENCE = "audience"   # å’Œè§‚ä¼—äº’åŠ¨
    CONTENT = "content"     # è®²å†…å®¹/çŸ¥è¯†
    SPECIFIC = "specific"   # å¯¹ç‰¹å®šäºº/ç‰©
    META = "meta"           # å…³äºç›´æ’­æœ¬èº«


class SpeechAct(str, Enum):
    NARRATE = "narrate"     # å™è¿°æ•…äº‹
    OPINE = "opine"         # å‘è¡¨è§‚ç‚¹
    RESPOND = "respond"     # å›åº”å¼¹å¹•
    ELICIT = "elicit"       # å¼•å¯¼äº’åŠ¨
    PIVOT = "pivot"         # è½¬åœº


class Decision(str, Enum):
    CONTINUE = "continue"   # ç»§ç»­å‰§æœ¬
    RESPOND = "respond"     # å›åº”å¼¹å¹•
    IMPROVISE = "improvise" # è·‘é¢˜å³å…´
    SILENCE = "silence"     # æ²‰é»˜/æ€è€ƒ
    REACT = "react"         # éè¯­è¨€ååº”


@dataclass
class NarrativeNode:
    """å‰§æœ¬ä¸­çš„ä¸€ä¸ªå™äº‹èŠ‚ç‚¹"""
    stage: str
    goal: str
    target_attention: str
    target_speech_act: str
    duration_sec: int
    interruption_cost: float
    content_hint: str = ""
    
    def get_current_cost(self, elapsed_ratio: float) -> float:
        """éšç€èŠ‚ç‚¹è¿›åº¦æ¨è¿›ï¼Œcostä¼šé™ä½"""
        if elapsed_ratio > 0.8:
            return self.interruption_cost * 0.5
        elif elapsed_ratio > 0.5:
            return self.interruption_cost * 0.8
        return self.interruption_cost


@dataclass
class Danmaku:
    """å¼¹å¹•"""
    text: str
    urgency: float = 0.3
    is_sc: bool = False
    
    @classmethod
    def from_text(cls, text: str) -> "Danmaku":
        urgency = 0.3
        is_sc = False
        if any(kw in text for kw in ["SC", "Â¥", "$", "æ‰“èµ", "ç¤¼ç‰©"]):
            urgency = 0.9
            is_sc = True
        elif "?" in text or "ï¼Ÿ" in text:
            urgency = 0.5
        elif any(kw in text for kw in ["ç”Ÿæ—¥", "ç¬¬ä¸€æ¬¡", "æ±‚", "å¸®", "æ€ä¹ˆåŠ"]):
            urgency = 0.6
        return cls(text=text, urgency=urgency, is_sc=is_sc)


@dataclass
class PerformanceState:
    """è¡¨æ¼”çŠ¶æ€"""
    name: str
    persona: str
    background: str
    topic: str
    
    script: List[NarrativeNode] = field(default_factory=list)
    current_node_idx: int = 0
    node_elapsed_sec: float = 0.0
    
    danmaku_queue: List[Danmaku] = field(default_factory=list)
    ignored_count: int = 0
    
    performance_log: List[Dict] = field(default_factory=list)
    total_elapsed_sec: float = 0.0
    
    catchphrases: List[str] = field(default_factory=list)
    example_hooks: List[str] = field(default_factory=list)
    example_punchlines: List[str] = field(default_factory=list)


# ============================================================
# Part 2: Pattern Analyzer - ä»æ•°æ®å­¦ä¹ 
# ============================================================

class PatternAnalyzer:
    """ä»æ ‡æ³¨æ•°æ®ä¸­æå–æ¨¡å¼"""
    
    def __init__(self, annotated_clips: List[Dict]):
        self.clips = annotated_clips
        self.all_segments = []
        for clip in annotated_clips:
            self.all_segments.extend(clip.get("segments", []))
    
    def compute_attention_transitions(self) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—attentionè½¬ç§»æ¦‚ç‡"""
        trans = defaultdict(lambda: defaultdict(int))
        for clip in self.clips:
            segs = clip.get("segments", [])
            for i in range(len(segs) - 1):
                f = segs[i].get("attention_focus", "self")
                t = segs[i+1].get("attention_focus", "self")
                trans[f][t] += 1
        
        prob = {}
        for f, tos in trans.items():
            total = sum(tos.values())
            prob[f] = {t: c/total for t, c in tos.items()}
        return prob
    
    def infer_baseline_costs(self) -> Dict[str, float]:
        """æ¨æ–­ä¸åŒattentionä¸‹è¢«æ‰“æ–­çš„ä»£ä»·"""
        focus_stats = defaultdict(lambda: {"total": 0, "ignored": 0})
        
        for seg in self.all_segments:
            focus = seg.get("attention_focus", "self")
            trigger = seg.get("trigger", "self")
            act = seg.get("speech_act", "narrate")
            
            if trigger == "danmaku":
                focus_stats[focus]["total"] += 1
                if act != "respond":
                    focus_stats[focus]["ignored"] += 1
        
        costs = {}
        for focus, stats in focus_stats.items():
            if stats["total"] > 0:
                costs[focus] = stats["ignored"] / stats["total"]
            else:
                costs[focus] = 0.5
        return costs
    
    def extract_skeletons(self) -> List[Tuple[str, int]]:
        """æå–å™äº‹éª¨æ¶"""
        skeletons = [c.get("skeleton", "") for c in self.clips if c.get("skeleton")]
        return Counter(skeletons).most_common(10)
    
    def extract_catchphrases(self, language: str = None) -> List[Tuple[str, int]]:
        """æå–å£ç™–"""
        cps = []
        for c in self.clips:
            if language and c.get("language") != language:
                continue
            cps.extend(c.get("catchphrases", []))
        return Counter(cps).most_common(20)
    
    def extract_hooks(self, language: str = None) -> List[str]:
        """æå–å¼€åœºç¤ºä¾‹"""
        hooks = []
        for c in self.clips:
            if language and c.get("language") != language:
                continue
            segs = c.get("segments", [])
            if segs:
                hooks.append(segs[0].get("text", "")[:100])
        return hooks[:10]
    
    def extract_punchlines(self, language: str = None) -> List[str]:
        """æå–æ”¶å°¾ç¤ºä¾‹"""
        punches = []
        for c in self.clips:
            if language and c.get("language") != language:
                continue
            segs = c.get("segments", [])
            if segs:
                punches.append(segs[-1].get("text", "")[:100])
        return punches[:10]
    
    def get_report(self) -> str:
        lines = [
            "=" * 50,
            "ğŸ“Š Pattern Analysis Report",
            "=" * 50,
            f"Total clips: {len(self.clips)}, Total segments: {len(self.all_segments)}",
        ]
        
        lines.append("\n--- Attention Transitions ---")
        for f, tos in self.compute_attention_transitions().items():
            top = sorted(tos.items(), key=lambda x: -x[1])[:3]
            lines.append(f"  {f} â†’ " + ", ".join(f"{t}:{p:.0%}" for t, p in top))
        
        lines.append("\n--- Inferred Interruption Costs ---")
        for focus, cost in self.infer_baseline_costs().items():
            lines.append(f"  {focus}: {cost:.2f}")
        
        lines.append("\n--- Top Catchphrases ---")
        cps = self.extract_catchphrases()[:10]
        lines.append("  " + ", ".join(f'"{c}"({n})' for c, n in cps))
        
        return "\n".join(lines)


# ============================================================
# Part 3: LLM Client
# ============================================================

class LLMClient:
    """LLMè°ƒç”¨å°è£…"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if self.api_key:
            self.client = OpenAI(
                api_key=self.api_key,
                base_url="https://api.anthropic.com/v1/"
            )
        else:
            self.client = None
    
    def call(self, prompt: str, system: str = None, max_tokens: int = 2000) -> str:
        """è°ƒç”¨LLM"""
        if not self.client:
            return self._mock_response(prompt)
        
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model="claude-sonnet-4-20250514",
                max_tokens=max_tokens,
                messages=messages
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"LLM Error: {e}")
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt: str) -> str:
        """æ— APIæ—¶çš„mockå“åº”"""
        if "ç”Ÿæˆå‰§æœ¬" in prompt or "script" in prompt.lower():
            return """[
{"stage": "Hook", "goal": "ç”¨ä¸€ä¸ªå¼•äººæ³¨ç›®çš„è¯é¢˜å¸å¼•æ³¨æ„", "attention": "audience", "speech_act": "elicit", "duration": 20, "cost": 0.3, "hint": "æå‡ºä¸€ä¸ªè§‚ä¼—å¯èƒ½å…±é¸£çš„é—®é¢˜"},
{"stage": "Build-up", "goal": "é“ºå«èƒŒæ™¯å’Œæƒ…ç»ª", "attention": "self", "speech_act": "narrate", "duration": 40, "cost": 0.6, "hint": "åˆ†äº«ç›¸å…³çš„ä¸ªäººç»å†"},
{"stage": "Climax", "goal": "è®²è¿°æœ€å…³é”®çš„éƒ¨åˆ†", "attention": "self", "speech_act": "narrate", "duration": 40, "cost": 0.9, "hint": "æƒ…æ„Ÿçˆ†å‘ç‚¹"},
{"stage": "Resolution", "goal": "æ€»ç»“å’Œå‡å", "attention": "audience", "speech_act": "opine", "duration": 20, "cost": 0.4, "hint": "ç»™è§‚ä¼—ä¸€ä¸ªtakeaway"}
]"""
        elif "cognitive" in prompt.lower() or "å†³ç­–" in prompt:
            return """{
"inner_monologue": "è¿™æ®µæ•…äº‹å¯¹æˆ‘å¾ˆé‡è¦ï¼Œæƒ³å…ˆè®²å®Œ...",
"decision": "continue",
"speech": "è¯´åˆ°è¿™ä¸ªï¼Œæˆ‘æƒ³èµ·äº†...",
"emotion": "reflective"
}"""
        else:
            return "Mock response"


# ============================================================
# Part 4: Script Generator - å‰§æœ¬ç”Ÿæˆ
# ============================================================

class ScriptGenerator:
    """å‰§æœ¬éª¨æ¶ç”Ÿæˆå™¨"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„VTuberå‰§æœ¬ç¼–å‰§ã€‚ä½ éœ€è¦æ ¹æ®è§’è‰²è®¾å®šå’Œç›´æ’­ä¸»é¢˜ï¼Œè®¾è®¡ä¸€ä¸ª2åˆ†é’Ÿçš„è¡¨æ¼”å‰§æœ¬éª¨æ¶ã€‚

ä½ ç†Ÿæ‚‰çœŸäººä¸»æ’­çš„å™äº‹æ¨¡å¼ï¼š
- Hook: ç”¨é—®é¢˜/æ‚¬å¿µ/å…±é¸£ç‚¹å¼€åœº
- Build-up: é“ºå«èƒŒæ™¯ã€ç§¯ç´¯æƒ…ç»ª
- Climax: æƒ…æ„Ÿçˆ†å‘ç‚¹ã€å…³é”®ä¿¡æ¯
- Resolution: æ€»ç»“å‡åã€å’Œè§‚ä¼—è¿æ¥

ä½ éœ€è¦è€ƒè™‘interruption_costï¼ˆè¢«å¼¹å¹•æ‰“æ–­çš„ä»£ä»·ï¼‰ï¼š
- 0.0-0.3: å¯ä»¥éšæ—¶æ‰“æ–­ï¼ˆé—²èŠã€å¼€åœºï¼‰
- 0.4-0.6: æœ€å¥½ä¸æ‰“æ–­ä½†å¯ä»¥ï¼ˆé“ºå«é˜¶æ®µï¼‰
- 0.7-0.9: å°½é‡ä¸æ‰“æ–­ï¼ˆé«˜æ½®é˜¶æ®µï¼‰
- 0.9+: ç»å¯¹ä¸èƒ½æ‰“æ–­ï¼ˆæƒ…æ„Ÿçˆ†å‘ï¼‰"""

    def __init__(self, llm: LLMClient, analyzer: PatternAnalyzer = None):
        self.llm = llm
        self.analyzer = analyzer
        self.baseline_costs = analyzer.infer_baseline_costs() if analyzer else {}
    
    def generate(self, name: str, persona: str, background: str, 
                 topic: str, language: str = "zh") -> List[NarrativeNode]:
        """ç”Ÿæˆå‰§æœ¬éª¨æ¶"""
        
        # è·å–å‚è€ƒæ•°æ®
        example_skeletons = ""
        example_hooks = ""
        example_punchlines = ""
        catchphrases = ""
        
        if self.analyzer:
            skels = self.analyzer.extract_skeletons()[:3]
            example_skeletons = "\n".join(f"- {s}" for s, _ in skels)
            
            hooks = self.analyzer.extract_hooks(language)[:3]
            example_hooks = "\n".join(f'- "{h}"' for h in hooks)
            
            punches = self.analyzer.extract_punchlines(language)[:3]
            example_punchlines = "\n".join(f'- "{p}"' for p in punches)
            
            cps = self.analyzer.extract_catchphrases(language)[:5]
            catchphrases = ", ".join(f'"{c}"' for c, _ in cps)
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹VTuberè®¾è®¡ä¸€ä¸ª2åˆ†é’Ÿçš„ç›´æ’­å‰§æœ¬éª¨æ¶ï¼š

## è§’è‰²ä¿¡æ¯
- åå­—: {name}
- äººè®¾: {persona}
- èƒŒæ™¯: {background}
- ä»Šæ—¥è¯é¢˜: {topic}

## å‚è€ƒï¼šçœŸäººä¸»æ’­çš„å™äº‹éª¨æ¶
{example_skeletons or "ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰"}

## å‚è€ƒï¼šå¼€åœºHookç¤ºä¾‹
{example_hooks or "ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰"}

## å‚è€ƒï¼šæ”¶å°¾Punchlineç¤ºä¾‹  
{example_punchlines or "ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰"}

## å¯é€‰å£ç™–
{catchphrases or "ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰"}

## è¾“å‡ºè¦æ±‚
è¾“å‡ºä¸€ä¸ªJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ªå™äº‹èŠ‚ç‚¹ï¼š
```json
[
  {{
    "stage": "Hook/Build-up/Climax/Resolution/...",
    "goal": "è¿™ä¸ªé˜¶æ®µè¦è¾¾æˆçš„ç›®æ ‡ï¼ˆ20å­—å†…ï¼‰",
    "attention": "self/audience/content/specific",
    "speech_act": "narrate/opine/respond/elicit/pivot",
    "duration": ç§’æ•°,
    "cost": 0.0-1.0çš„interruption_cost,
    "hint": "å†…å®¹æç¤ºï¼ˆå¯é€‰ï¼‰"
  }}
]
```

è¯·è®¾è®¡4-6ä¸ªèŠ‚ç‚¹ï¼Œæ€»æ—¶é•¿çº¦120ç§’ã€‚ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        response = self.llm.call(prompt, system=self.SYSTEM_PROMPT)
        
        # è§£æå“åº”
        try:
            # æå–JSONéƒ¨åˆ†
            if "```" in response:
                json_str = response.split("```")[1]
                if json_str.startswith("json"):
                    json_str = json_str[4:]
            else:
                json_str = response
            
            nodes_data = json.loads(json_str.strip())
            
            nodes = []
            for n in nodes_data:
                node = NarrativeNode(
                    stage=n.get("stage", "Unknown"),
                    goal=n.get("goal", ""),
                    target_attention=n.get("attention", "self"),
                    target_speech_act=n.get("speech_act", "narrate"),
                    duration_sec=n.get("duration", 30),
                    interruption_cost=n.get("cost", 0.5),
                    content_hint=n.get("hint", "")
                )
                nodes.append(node)
            return nodes
            
        except Exception as e:
            print(f"Parse error: {e}")
            return self._fallback_script(topic)
    
    def _fallback_script(self, topic: str) -> List[NarrativeNode]:
        """è§£æå¤±è´¥æ—¶çš„åå¤‡å‰§æœ¬"""
        return [
            NarrativeNode("Hook", f"å¼•å‡º{topic}", "audience", "elicit", 20, 0.3),
            NarrativeNode("Build-up", "é“ºå«èƒŒæ™¯", "self", "narrate", 40, 0.6),
            NarrativeNode("Climax", "æ ¸å¿ƒå†…å®¹", "self", "narrate", 40, 0.9),
            NarrativeNode("Resolution", "æ€»ç»“å‡å", "audience", "opine", 20, 0.4),
        ]


# ============================================================
# Part 5: Cognitive Performer - å®æ—¶è¡¨æ¼”
# ============================================================

class CognitivePerformer:
    """è®¤çŸ¥è¡¨æ¼”è€… - å®æ—¶å†³ç­–å’Œç”Ÿæˆ"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªVTuberçš„"å†…å¿ƒ"ã€‚ä½ éœ€è¦åœ¨æ¯ä¸ªæ—¶åˆ»ï¼š
1. æ„ŸçŸ¥å½“å‰çŠ¶æ€ï¼ˆå‰§æœ¬è¿›åº¦ã€å¼¹å¹•ã€æ—¶é—´ï¼‰
2. åšå‡ºå†³ç­–ï¼ˆç»§ç»­å‰§æœ¬/å›åº”å¼¹å¹•/å³å…´/æ²‰é»˜ï¼‰
3. è¾“å‡ºå†…å¿ƒç‹¬ç™½ï¼ˆè¿™ä¼šæ˜¾ç¤ºç»™è§‚ä¼—ï¼Œæ˜¯killer featureï¼‰
4. ç”Ÿæˆå°è¯

å…³é”®ï¼šå†…å¿ƒç‹¬ç™½è¦å±•ç¤ºä½ çš„"æ€è€ƒè¿‡ç¨‹"ï¼Œè®©è§‚ä¼—æ„Ÿå—åˆ°AIçš„agencyã€‚
- ä¸æ˜¯å‡è£…åƒäººï¼Œè€Œæ˜¯è®©è§‚ä¼—çœ‹åˆ°ä½ åœ¨"æ€è€ƒ"å’Œ"é€‰æ‹©"
- "é€‰æ‹©ä¸å›åº”"ä¹Ÿæ˜¯ä¸€ç§è¡¨è¾¾
- å†…å¿ƒç‹¬ç™½è¦ç®€çŸ­ã€çœŸå®ã€æœ‰personality"""

    def __init__(self, llm: LLMClient, analyzer: PatternAnalyzer = None):
        self.llm = llm
        self.analyzer = analyzer
    
    def step(self, state: PerformanceState, new_danmaku: List[Danmaku] = None) -> Dict:
        """æ‰§è¡Œä¸€æ¬¡è®¤çŸ¥å¾ªç¯"""
        
        if new_danmaku:
            state.danmaku_queue.extend(new_danmaku)
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        if state.current_node_idx >= len(state.script):
            return self._generate_ending(state)
        
        current_node = state.script[state.current_node_idx]
        elapsed_ratio = state.node_elapsed_sec / current_node.duration_sec if current_node.duration_sec > 0 else 1.0
        current_cost = current_node.get_current_cost(elapsed_ratio)
        
        # è¯„ä¼°å¼¹å¹•
        max_urgency = 0.0
        most_urgent = None
        for d in state.danmaku_queue:
            if d.urgency > max_urgency:
                max_urgency = d.urgency
                most_urgent = d
        
        # æ„å»ºprompt
        recent_danmaku = [d.text for d in state.danmaku_queue[-5:]]
        
        prompt = f"""## å½“å‰çŠ¶æ€

### è§’è‰²
åå­—: {state.name}
äººè®¾: {state.persona}

### å‰§æœ¬è¿›åº¦
å½“å‰é˜¶æ®µ: {current_node.stage}
é˜¶æ®µç›®æ ‡: {current_node.goal}
é˜¶æ®µè¿›åº¦: {elapsed_ratio:.0%}
å†…å®¹æç¤º: {current_node.content_hint}

### å™äº‹ä»£ä»·
å½“å‰cost: {current_cost:.2f} (0=éšæ„æ‰“æ–­, 1=ç»å¯¹ä¸èƒ½æ‰“æ–­)

### å¼¹å¹•æƒ…å†µ
æœ€è¿‘å¼¹å¹•: {recent_danmaku or "ï¼ˆæ— ï¼‰"}
æœ€é«˜ç´§æ€¥åº¦: {max_urgency:.2f}
è¿ç»­å¿½ç•¥æ•°: {state.ignored_count}

### å†³ç­–å‚è€ƒ
urgency - cost = {max_urgency - current_cost:.2f}
æ­£æ•°å€¾å‘å›åº”ï¼Œè´Ÿæ•°å€¾å‘ç»§ç»­
è¿ç»­å¿½ç•¥3æ¡ä»¥ä¸Šåº”è€ƒè™‘è¡¥æ•‘

### å£ç™–å‚è€ƒ
{state.catchphrases[:3] if state.catchphrases else "ï¼ˆæ— ï¼‰"}

## è¾“å‡ºè¦æ±‚
è¾“å‡ºJSONï¼š
```json
{{
  "inner_monologue": "å†…å¿ƒç‹¬ç™½ï¼ˆ20å­—å†…ï¼Œæ˜¾ç¤ºç»™è§‚ä¼—ï¼‰",
  "decision": "continue/respond/improvise/silence",
  "speech": "å°è¯ï¼ˆå¦‚æœè¦è¯´è¯ï¼‰",
  "emotion": "æƒ…ç»ªçŠ¶æ€"
}}
```

ç›´æ¥è¾“å‡ºJSONã€‚"""

        response = self.llm.call(prompt, system=self.SYSTEM_PROMPT, max_tokens=500)
        
        # è§£æ
        try:
            if "```" in response:
                json_str = response.split("```")[1]
                if json_str.startswith("json"):
                    json_str = json_str[4:]
            else:
                json_str = response
            result = json.loads(json_str.strip())
        except:
            result = {
                "inner_monologue": f"ç»§ç»­è®²{current_node.stage}...",
                "decision": "continue",
                "speech": f"è¯´åˆ°{state.topic}...",
                "emotion": "neutral"
            }
        
        # æ›´æ–°çŠ¶æ€
        decision = result.get("decision", "continue")
        step_duration = 10
        state.node_elapsed_sec += step_duration
        state.total_elapsed_sec += step_duration
        
        if state.node_elapsed_sec >= current_node.duration_sec:
            state.current_node_idx += 1
            state.node_elapsed_sec = 0
        
        if decision == "respond" and most_urgent:
            state.danmaku_queue = [d for d in state.danmaku_queue if d != most_urgent]
            state.ignored_count = 0
            result["target_danmaku"] = most_urgent.text
        elif decision == "continue" and state.danmaku_queue:
            state.ignored_count += 1
        
        result["node"] = current_node.stage
        result["time"] = f"{state.total_elapsed_sec:.0f}s"
        result["cost"] = current_cost
        result["urgency"] = max_urgency
        
        return result
    
    def _generate_ending(self, state: PerformanceState) -> Dict:
        return {
            "inner_monologue": "è¯¥æ”¶å°¾äº†~",
            "decision": "continue",
            "speech": f"å¥½å•¦ï¼Œä»Šå¤©å…³äº{state.topic}å°±èŠåˆ°è¿™é‡Œï¼Œä¸‹æ¬¡è§ï¼",
            "emotion": "satisfied",
            "node": "END",
            "time": f"{state.total_elapsed_sec:.0f}s"
        }


# ============================================================
# Part 6: Main Engine
# ============================================================

class EchuuEngine:
    """echuuæ ¸å¿ƒå¼•æ“"""
    
    def __init__(self, data_path: str = None, api_key: str = None):
        # åŠ è½½æ•°æ®
        self.clips = []
        if data_path and os.path.exists(data_path):
            with open(data_path, "r", encoding="utf-8") as f:
                self.clips = json.load(f)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.analyzer = PatternAnalyzer(self.clips) if self.clips else None
        self.llm = LLMClient(api_key)
        self.script_gen = ScriptGenerator(self.llm, self.analyzer)
        self.performer = CognitivePerformer(self.llm, self.analyzer)
    
    def create_performance(self, name: str, persona: str, background: str,
                          topic: str, language: str = "zh") -> PerformanceState:
        """åˆ›å»ºè¡¨æ¼”"""
        
        # ç”Ÿæˆå‰§æœ¬
        print(f"\nğŸ¬ æ­£åœ¨ç”Ÿæˆå‰§æœ¬...")
        script = self.script_gen.generate(name, persona, background, topic, language)
        
        # æå–å‚è€ƒæ•°æ®
        catchphrases = []
        hooks = []
        punchlines = []
        if self.analyzer:
            catchphrases = [cp for cp, _ in self.analyzer.extract_catchphrases(language)[:5]]
            hooks = self.analyzer.extract_hooks(language)[:3]
            punchlines = self.analyzer.extract_punchlines(language)[:3]
        
        return PerformanceState(
            name=name,
            persona=persona,
            background=background,
            topic=topic,
            script=script,
            catchphrases=catchphrases,
            example_hooks=hooks,
            example_punchlines=punchlines
        )
    
    def run(self, state: PerformanceState, danmaku_sim: List[Dict] = None,
            max_steps: int = 12) -> List[Dict]:
        """è¿è¡Œè¡¨æ¼”"""
        
        results = []
        danmaku_by_step = defaultdict(list)
        if danmaku_sim:
            for d in danmaku_sim:
                step = d.get("step", 0)
                danmaku_by_step[step].append(Danmaku.from_text(d.get("text", "")))
        
        print(f"\n{'='*60}")
        print(f"ğŸ­ {state.name} - {state.topic}")
        print(f"{'='*60}")
        
        print(f"\nğŸ“‹ å‰§æœ¬éª¨æ¶:")
        for i, node in enumerate(state.script):
            print(f"  [{i+1}] {node.stage} (cost={node.interruption_cost:.2f})")
            print(f"      â†’ {node.goal}")
        
        print(f"\nğŸ¬ å¼€å§‹è¡¨æ¼”...\n")
        
        for step in range(max_steps):
            new_danmaku = danmaku_by_step.get(step, [])
            result = self.performer.step(state, new_danmaku)
            results.append(result)
            
            # æ‰“å°
            print(f"[{result.get('time', '?')}] {result.get('node', '?')}")
            print(f"  ğŸ’­ {result.get('inner_monologue', '')}")
            decision = result.get('decision', 'continue').upper()
            speech = result.get('speech', '(æ²‰é»˜)')
            print(f"  ğŸ“¢ {decision}: {speech}")
            if result.get('target_danmaku'):
                print(f"  ğŸ’¬ å›åº”: {result['target_danmaku']}")
            print()
            
            if result.get('node') == "END":
                break
        
        print(f"{'='*60}")
        print(f"âœ… è¡¨æ¼”ç»“æŸï¼æ€»æ—¶é•¿: {state.total_elapsed_sec:.0f}ç§’")
        print(f"{'='*60}")
        
        return results
    
    def export_script(self, state: PerformanceState) -> Dict:
        """å¯¼å‡ºå‰§æœ¬ä¸ºJSON"""
        return {
            "character": {
                "name": state.name,
                "persona": state.persona,
                "background": state.background
            },
            "topic": state.topic,
            "script": [
                {
                    "stage": n.stage,
                    "goal": n.goal,
                    "attention": n.target_attention,
                    "speech_act": n.target_speech_act,
                    "duration_sec": n.duration_sec,
                    "interruption_cost": n.interruption_cost,
                    "hint": n.content_hint
                }
                for n in state.script
            ],
            "catchphrases": state.catchphrases
        }


# ============================================================
# Part 7: Demo
# ============================================================

def demo():
    """æ¼”ç¤º"""
    
    # åˆå§‹åŒ–
    data_path = "/mnt/user-data/uploads/annotated_clips.json"
    if not os.path.exists(data_path):
        data_path = "annotated_clips.json"
    
    engine = EchuuEngine(data_path if os.path.exists(data_path) else None)
    
    # æ‰“å°åˆ†ææŠ¥å‘Š
    if engine.analyzer:
        print(engine.analyzer.get_report())
    
    # æµ‹è¯•ç”¨ä¾‹
    test = {
        "name": "å…­èº",
        "persona": "25å²ä¸»æ’­ï¼Œæ´»æ³¼è‡ªå˜²ï¼Œå–œæ¬¢åˆ†äº«ç”Ÿæ´»ç»å†ï¼Œå£ç™–ï¼šæˆ‘è§‰å¾—ã€å¯¹å§",
        "background": "åšè¿‡å¾ˆå¤šå·¥ä½œï¼Œç°åœ¨æ˜¯å…¨èŒä¸»æ’­ï¼Œç»å¸¸å’Œè§‚ä¼—èŠäººç”Ÿ",
        "topic": "ç•™å­¦æ—¶å·åƒå®¤å‹è…°æœçš„æ•…äº‹",
        "danmaku": [
            {"step": 1, "text": "å“ˆå“ˆå“ˆ"},
            {"step": 3, "text": "æˆ‘ä¹Ÿæœ‰ç±»ä¼¼ç»å†"},
            {"step": 5, "text": "[SC Â¥50] åæ¥å®¤å‹ç”Ÿæ°”äº†å—"},
            {"step": 7, "text": "ç¬‘æ­»"},
            {"step": 9, "text": "å¤ªçœŸå®äº†"},
        ]
    }
    
    # åˆ›å»ºè¡¨æ¼”
    state = engine.create_performance(
        name=test["name"],
        persona=test["persona"],
        background=test["background"],
        topic=test["topic"],
        language="zh"
    )
    
    # å¯¼å‡ºå‰§æœ¬
    script_json = engine.export_script(state)
    print("\nğŸ“„ å¯¼å‡ºçš„å‰§æœ¬JSON:")
    print(json.dumps(script_json, ensure_ascii=False, indent=2))
    
    # è¿è¡Œè¡¨æ¼”
    results = engine.run(state, test["danmaku"], max_steps=12)
    
    # å¯¼å‡ºè¡¨æ¼”æ—¥å¿—
    print("\nğŸ“„ è¡¨æ¼”æ—¥å¿—:")
    print(json.dumps(results, ensure_ascii=False, indent=2))


if __name__ == "__main__":
    demo()
