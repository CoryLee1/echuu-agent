"""
VTuberæ•°æ®æ ‡æ³¨Pipeline
å°†åŸå§‹JSONLæ•°æ®è½¬æ¢ä¸ºechuu notebookå¯ç”¨çš„annotatedæ ¼å¼
"""

import json
import re
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Tuple
from enum import Enum
from pathlib import Path

# ============================================
# Schemaå®šä¹‰ï¼ˆä¸echuu notebookä¿æŒä¸€è‡´ï¼‰
# ============================================

class AttentionFocus(str, Enum):
    SELF = "self"           # è‡ªè¨€è‡ªè¯­/å†…å¿ƒç‹¬ç™½/è®²è‡ªå·±çš„äº‹
    AUDIENCE = "audience"   # ç›´æ¥å¯¹è§‚ä¼—è¯´è¯ï¼ˆä½ ä»¬ã€å¤§å®¶ï¼‰
    SPECIFIC = "specific"   # å›åº”ç‰¹å®šè§‚ä¼—ï¼ˆè¯»SCã€ç‚¹åï¼‰
    CONTENT = "content"     # ä¸“æ³¨äºå†…å®¹ï¼ˆé€—çŒ«ã€çœ‹ç”»é¢ï¼‰
    META = "meta"           # è°ˆè®ºç›´æ’­æœ¬èº«


class SpeechAct(str, Enum):
    NARRATE = "narrate"     # å™äº‹ï¼šè®²æ•…äº‹ã€æè¿°ç»å†
    OPINE = "opine"         # è¡¨æ€ï¼šå‘è¡¨è§‚ç‚¹ã€è¯„ä»·
    RESPOND = "respond"     # å›åº”ï¼šå›ç­”é—®é¢˜ã€æ¥æ¢—
    ELICIT = "elicit"       # å¼•å‡ºï¼šæé—®ã€æŠ›æ¢—ã€é‚€è¯·äº’åŠ¨
    PIVOT = "pivot"         # è½¬æŠ˜ï¼šè¯é¢˜è½¬æ¢ã€æ‰¿ä¸Šå¯ä¸‹
    BACKCHANNEL = "backchannel"  # å¡«å……ï¼šè¯­æ°”è¯ã€æ€è€ƒã€è¿‡æ¸¡


class Trigger(str, Enum):
    SC = "sc"               # SC/æ‰“èµè§¦å‘
    DANMAKU = "danmaku"     # å¼¹å¹•è§¦å‘
    SELF = "self"           # è‡ªå‘ï¼ˆæ— å¤–éƒ¨è§¦å‘ï¼‰
    CONTENT = "content"     # å†…å®¹è§¦å‘ï¼ˆçŒ«åŠ¨äº†ã€ç”»é¢å˜åŒ–ï¼‰
    PRIOR = "prior"         # æ‰¿æ¥ä¸Šæ–‡


@dataclass
class Segment:
    """æ ‡æ³¨åçš„segment"""
    id: str
    start_time: float
    end_time: float
    text: str
    attention_focus: str
    speech_act: str
    trigger: str
    catchphrase: Optional[str] = None
    emotion_shift: bool = False
    
    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "text": self.text,
            "attention_focus": self.attention_focus,
            "speech_act": self.speech_act,
            "trigger": self.trigger,
            "catchphrase": self.catchphrase,
            "emotion_shift": self.emotion_shift
        }


@dataclass
class AnnotatedClip:
    """æ ‡æ³¨åçš„clip"""
    clip_id: str
    source: str
    language: str
    duration: float
    title: str
    skeleton: str
    catchphrases: List[str]
    segments: List[Segment]
    notes: Optional[Dict] = None
    
    def to_dict(self) -> dict:
        return {
            "clip_id": self.clip_id,
            "source": self.source,
            "language": self.language,
            "duration": self.duration,
            "title": self.title,
            "skeleton": self.skeleton,
            "catchphrases": self.catchphrases,
            "segments": [s.to_dict() for s in self.segments],
            "notes": self.notes
        }


# ============================================
# æ•°æ®åŠ è½½
# ============================================

def load_raw_clips(jsonl_path: str) -> List[Dict]:
    """åŠ è½½åŸå§‹JSONLæ•°æ®"""
    clips = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                clips.append(json.loads(line))
    return clips


def extract_catchphrases(habit_str: str) -> List[str]:
    """ä»notes.habitå­—ç¬¦ä¸²ä¸­æå–å£ç™–åˆ—è¡¨"""
    if not habit_str:
        return []
    # åŒ¹é…å¼•å·å†…çš„å†…å®¹
    pattern = r'["""]([^"""]+)["""]'
    matches = re.findall(pattern, habit_str)
    # æ¸…ç†å¹¶å»é‡
    catchphrases = []
    for m in matches:
        clean = m.strip()
        if clean and clean not in catchphrases:
            catchphrases.append(clean)
    return catchphrases


def extract_skeleton(structure_str: str) -> str:
    """æå–å™äº‹éª¨æ¶"""
    if not structure_str:
        return ""
    # å·²ç»æ˜¯ç®­å¤´æ ¼å¼çš„ç›´æ¥è¿”å›
    if "â†’" in structure_str or "->" in structure_str:
        return structure_str.replace("->", "â†’")
    return structure_str


def infer_trigger_from_notes(notes: Dict) -> str:
    """ä»notesæ¨æ–­ä¸»è¦è§¦å‘ç±»å‹"""
    trigger_text = notes.get("trigger", "").lower()
    
    if any(kw in trigger_text for kw in ["sc", "æ‰“èµ", "ç¤¼ç‰©", "superchat"]):
        return "sc"
    elif any(kw in trigger_text for kw in ["å¼¹å¹•", "è§‚ä¼—", "é—®é¢˜", "æé—®", "chat"]):
        return "danmaku"
    elif any(kw in trigger_text for kw in ["å® ç‰©", "çŒ«", "ç‹—", "ç”»é¢", "å†…å®¹"]):
        return "content"
    elif any(kw in trigger_text for kw in ["è‡ªå‘", "é—²èŠ", "å›å¿†", "åˆ†äº«"]):
        return "self"
    else:
        return "self"


# ============================================
# LLMæ ‡æ³¨Prompt
# ============================================

SEGMENT_ANNOTATION_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç›´æ’­å†…å®¹åˆ†æå¸ˆã€‚è¯·å¯¹ä»¥ä¸‹ç›´æ’­åˆ‡ç‰‡è¿›è¡Œsegmentçº§åˆ«çš„æ ‡æ³¨ã€‚

## åˆ‡ç‰‡ä¿¡æ¯
- æ ‡é¢˜: {title}
- è¯­è¨€: {language}
- æ—¶é•¿: {duration}ç§’
- ä¸»è¦è§¦å‘: {main_trigger}
- å™äº‹ç»“æ„: {skeleton}
- å£ç™–: {catchphrases}

## è½¬å½•æ–‡æœ¬
```
{transcript}
```

## åŸå§‹å¤‡æ³¨
{notes}

## æ ‡æ³¨ç»´åº¦

### 1. attention_focusï¼ˆæ³¨æ„åŠ›æŒ‡å‘ï¼‰
- `self`: è‡ªè¨€è‡ªè¯­/è®²è‡ªå·±çš„äº‹/å†…å¿ƒç‹¬ç™½
- `audience`: ç›´æ¥å¯¹è§‚ä¼—è¯´è¯ï¼ˆä½ ä»¬ã€å¤§å®¶ã€chatï¼‰
- `specific`: å›åº”ç‰¹å®šè§‚ä¼—ï¼ˆè¯»SCã€ç‚¹åã€æ„Ÿè°¢xxxï¼‰
- `content`: ä¸“æ³¨äºå†…å®¹ï¼ˆé€—çŒ«ã€çœ‹ç”»é¢ã€æè¿°åœºæ™¯ï¼‰
- `meta`: è°ˆè®ºç›´æ’­æœ¬èº«ï¼ˆä»Šå¤©æ’­å¤šä¹…ã€è®¾å¤‡é—®é¢˜ï¼‰

### 2. speech_actï¼ˆè¯è¯­è¡Œä¸ºï¼‰
- `narrate`: å™äº‹ï¼Œè®²æ•…äº‹ã€æè¿°ç»å†ã€å›å¿†
- `opine`: è¡¨æ€ï¼Œå‘è¡¨è§‚ç‚¹ã€è¯„ä»·ã€å»ºè®®
- `respond`: å›åº”ï¼Œå›ç­”é—®é¢˜ã€æ¥æ¢—ã€åé©³
- `elicit`: å¼•å‡ºï¼Œæé—®ã€æŠ›æ¢—ã€é‚€è¯·äº’åŠ¨
- `pivot`: è½¬æŠ˜ï¼Œè¯é¢˜è½¬æ¢ã€æ‰¿ä¸Šå¯ä¸‹
- `backchannel`: å¡«å……ï¼Œè¯­æ°”è¯ã€æ€è€ƒã€"å—¯"/"uhm"

### 3. triggerï¼ˆè§¦å‘æºï¼‰
- `sc`: SC/æ‰“èµè§¦å‘
- `danmaku`: å¼¹å¹•è§¦å‘
- `self`: è‡ªå‘ï¼ˆæ— å¤–éƒ¨è§¦å‘ï¼Œè‡ªå·±æƒ³èµ·æ¥çš„ï¼‰
- `content`: å†…å®¹è§¦å‘ï¼ˆç”»é¢å˜åŒ–ã€å® ç‰©åŠ¨ä½œï¼‰
- `prior`: æ‰¿æ¥ä¸Šæ–‡ï¼ˆå»¶ç»­å‰ä¸€ä¸ªsegmentï¼‰

## åˆ‡åˆ†åŸåˆ™

1. **è¯­ä¹‰å®Œæ•´**ï¼šæ¯ä¸ªsegmentåº”è¯¥æ˜¯ä¸€ä¸ªè¯­ä¹‰å®Œæ•´çš„å•å…ƒ
2. **æ—¶é•¿åˆç†**ï¼šé€šå¸¸5-30ç§’ï¼Œè¿‡é•¿éœ€è¦åˆ‡åˆ†
3. **å˜åŒ–æ•æ„Ÿ**ï¼šå½“attention_focusæˆ–speech_actæ˜æ˜¾å˜åŒ–æ—¶åˆ‡åˆ†
4. **ä¿æŒç®€æ´**ï¼šä¸€ä¸ª2åˆ†é’Ÿåˆ‡ç‰‡é€šå¸¸5-10ä¸ªsegment

## è¾“å‡ºæ ¼å¼

è¯·è¾“å‡ºJSONæ ¼å¼ï¼š

```json
{{
  "segments": [
    {{
      "id": "seg_01",
      "start_time": 0,
      "end_time": 15,
      "text": "å®Œæ•´çš„segmentæ–‡æœ¬",
      "attention_focus": "specific",
      "speech_act": "respond",
      "trigger": "sc",
      "catchphrase": "å¯¹å§",
      "emotion_shift": false
    }}
  ],
  "analysis_notes": "ç®€çŸ­åˆ†æå¤‡æ³¨"
}}
```

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚'''


def create_annotation_prompt(clip: Dict) -> str:
    """ä¸ºå•ä¸ªclipåˆ›å»ºæ ‡æ³¨prompt"""
    # æ ¼å¼åŒ–è½¬å½•æ–‡æœ¬
    transcript_lines = []
    for item in clip.get("transcript", []):
        t = item.get("t", 0)
        if t is not None:
            minutes = int(t // 60)
            seconds = int(t % 60)
            transcript_lines.append(f"{minutes}:{seconds:02d} {item.get('text', '')}")
        else:
            transcript_lines.append(f"-- {item.get('text', '')}")
    
    transcript = "\n".join(transcript_lines)
    
    # æå–ä¿¡æ¯
    notes = clip.get("notes", {})
    catchphrases = extract_catchphrases(notes.get("habit", ""))
    skeleton = extract_skeleton(notes.get("structure", ""))
    main_trigger = infer_trigger_from_notes(notes)
    
    # æ ¼å¼åŒ–notes
    notes_str = "\n".join([f"- {k}: {v}" for k, v in notes.items()])
    
    return SEGMENT_ANNOTATION_PROMPT.format(
        title=clip.get("title", ""),
        language=clip.get("language", "zh"),
        duration=clip.get("duration", 0),
        main_trigger=main_trigger,
        skeleton=skeleton,
        catchphrases=catchphrases,
        transcript=transcript,
        notes=notes_str
    )


# ============================================
# LLMè°ƒç”¨
# ============================================

def call_llm_for_annotation(prompt: str, client=None, model: str = "claude-3-haiku-20240307") -> Dict:
    """
    è°ƒç”¨LLMè¿›è¡Œæ ‡æ³¨
    
    Args:
        prompt: æ ‡æ³¨prompt
        client: Anthropic/OpenAI client
        model: æ¨¡å‹åç§°
    
    Returns:
        è§£æåçš„JSONç»“æœ
    """
    if client is None:
        print("âš ï¸ æ— API clientï¼Œè¿”å›ç©ºç»“æœ")
        return {"segments": [], "analysis_notes": "éœ€è¦é…ç½®API"}
    
    try:
        # Anthropic API
        if hasattr(client, 'messages'):
            response = client.messages.create(
                model=model,
                max_tokens=4096,
                messages=[{"role": "user", "content": prompt}]
            )
            content = response.content[0].text
        # OpenAI API
        else:
            response = client.chat.completions.create(
                model=model,
                max_tokens=4096,
                messages=[{"role": "user", "content": prompt}]
            )
            content = response.choices[0].message.content
        
        # æå–JSON
        if "```json" in content:
            json_str = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            json_str = content.split("```")[1].split("```")[0]
        else:
            json_str = content
        
        return json.loads(json_str.strip())
        
    except Exception as e:
        print(f"LLMè°ƒç”¨é”™è¯¯: {e}")
        return {"segments": [], "analysis_notes": f"é”™è¯¯: {e}"}


# ============================================
# æ•°æ®è½¬æ¢Pipeline
# ============================================

def annotate_clip(clip: Dict, client=None, model: str = "claude-3-haiku-20240307") -> AnnotatedClip:
    """
    æ ‡æ³¨å•ä¸ªclip
    
    Args:
        clip: åŸå§‹clipæ•°æ®
        client: LLM client
        model: æ¨¡å‹åç§°
    
    Returns:
        æ ‡æ³¨åçš„AnnotatedClip
    """
    notes = clip.get("notes", {})
    catchphrases = extract_catchphrases(notes.get("habit", ""))
    skeleton = extract_skeleton(notes.get("structure", ""))
    
    # åˆ›å»ºpromptå¹¶è°ƒç”¨LLM
    prompt = create_annotation_prompt(clip)
    result = call_llm_for_annotation(prompt, client, model)
    
    # è½¬æ¢segments
    segments = []
    for i, seg_data in enumerate(result.get("segments", [])):
        segment = Segment(
            id=seg_data.get("id", f"seg_{i+1:02d}"),
            start_time=seg_data.get("start_time", 0),
            end_time=seg_data.get("end_time", 0),
            text=seg_data.get("text", ""),
            attention_focus=seg_data.get("attention_focus", "self"),
            speech_act=seg_data.get("speech_act", "narrate"),
            trigger=seg_data.get("trigger", "self"),
            catchphrase=seg_data.get("catchphrase"),
            emotion_shift=seg_data.get("emotion_shift", False)
        )
        segments.append(segment)
    
    # å¦‚æœLLMæ²¡æœ‰è¿”å›segmentsï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤çš„
    if not segments:
        transcript = clip.get("transcript", [])
        if transcript:
            # å°†æ•´ä¸ªtranscriptä½œä¸ºä¸€ä¸ªsegment
            full_text = " ".join([t.get("text", "") for t in transcript])
            start_time = transcript[0].get("t", 0) or 0
            end_time = transcript[-1].get("t", 0) or clip.get("duration", 0)
            
            segments.append(Segment(
                id="seg_01",
                start_time=start_time,
                end_time=end_time,
                text=full_text[:500] + "..." if len(full_text) > 500 else full_text,
                attention_focus="self",
                speech_act="narrate",
                trigger=infer_trigger_from_notes(notes)
            ))
    
    return AnnotatedClip(
        clip_id=clip.get("clip_id", ""),
        source=clip.get("source", ""),
        language=clip.get("language", "zh"),
        duration=clip.get("duration", 0),
        title=clip.get("title", ""),
        skeleton=skeleton,
        catchphrases=catchphrases,
        segments=segments,
        notes=notes
    )


def run_annotation_pipeline(
    input_path: str,
    output_path: str,
    client=None,
    model: str = "claude-sonnet-4-20250514",
    max_clips: int = None,
    verbose: bool = True
) -> List[Dict]:
    """
    è¿è¡Œå®Œæ•´çš„æ ‡æ³¨Pipeline
    
    Args:
        input_path: è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        client: LLM client
        model: æ¨¡å‹åç§°
        max_clips: æœ€å¤§å¤„ç†clipæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¿›åº¦
    
    Returns:
        æ ‡æ³¨åçš„clipåˆ—è¡¨
    """
    # åŠ è½½æ•°æ®
    if verbose:
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    
    raw_clips = load_raw_clips(input_path)
    
    if max_clips:
        raw_clips = raw_clips[:max_clips]
    
    if verbose:
        print(f"âœ… åŠ è½½äº† {len(raw_clips)} ä¸ªclips")
    
    # æ ‡æ³¨
    annotated = []
    for i, clip in enumerate(raw_clips):
        if verbose:
            print(f"\n[{i+1}/{len(raw_clips)}] æ ‡æ³¨: {clip.get('title', 'Unknown')[:30]}...")
        
        try:
            annotated_clip = annotate_clip(clip, client, model)
            annotated.append(annotated_clip.to_dict())
            
            if verbose:
                print(f"  âœ“ {len(annotated_clip.segments)} segments, skeleton: {annotated_clip.skeleton[:30]}...")
        except Exception as e:
            print(f"  âœ— é”™è¯¯: {e}")
            # æ·»åŠ ä¸€ä¸ªæœ€å°åŒ–çš„æ ‡æ³¨
            annotated.append({
                "clip_id": clip.get("clip_id", ""),
                "source": clip.get("source", ""),
                "language": clip.get("language", ""),
                "duration": clip.get("duration", 0),
                "title": clip.get("title", ""),
                "skeleton": extract_skeleton(clip.get("notes", {}).get("structure", "")),
                "catchphrases": extract_catchphrases(clip.get("notes", {}).get("habit", "")),
                "segments": [],
                "notes": clip.get("notes", {})
            })
    
    # ä¿å­˜
    if verbose:
        print(f"\nğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(annotated, f, ensure_ascii=False, indent=2)
    
    if verbose:
        print(f"âœ… å®Œæˆï¼å…±æ ‡æ³¨ {len(annotated)} ä¸ªclips")
    
    return annotated


# ============================================
# å¿«é€Ÿè½¬æ¢ï¼ˆä¸è°ƒç”¨LLMï¼‰
# ============================================

def quick_convert_without_llm(
    input_path: str,
    output_path: str,
    verbose: bool = True
) -> List[Dict]:
    """
    å¿«é€Ÿè½¬æ¢æ•°æ®æ ¼å¼ï¼Œä¸è°ƒç”¨LLM
    ä½¿ç”¨å¯å‘å¼è§„åˆ™è¿›è¡Œsegmentåˆ‡åˆ†
    
    é€‚ç”¨äºï¼š
    1. å¿«é€Ÿæµ‹è¯•echuu notebook
    2. æ²¡æœ‰API keyæ—¶çš„fallback
    3. ç”Ÿæˆåˆç¨¿åäººå·¥æ ¡æ­£
    """
    if verbose:
        print(f"ğŸ“‚ å¿«é€Ÿè½¬æ¢æ¨¡å¼ï¼ˆä¸è°ƒç”¨LLMï¼‰")
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    
    raw_clips = load_raw_clips(input_path)
    
    if verbose:
        print(f"âœ… åŠ è½½äº† {len(raw_clips)} ä¸ªclips")
    
    annotated = []
    
    for clip in raw_clips:
        notes = clip.get("notes", {})
        catchphrases = extract_catchphrases(notes.get("habit", ""))
        skeleton = extract_skeleton(notes.get("structure", ""))
        main_trigger = infer_trigger_from_notes(notes)
        
        # å¯å‘å¼segmentåˆ‡åˆ†
        transcript = clip.get("transcript", [])
        segments = []
        
        # æŒ‰æ—¶é—´é—´éš”åˆ‡åˆ†ï¼ˆæ¯30ç§’å·¦å³ä¸€ä¸ªsegmentï¼‰
        current_seg_texts = []
        current_seg_start = 0
        last_time = 0
        
        for item in transcript:
            t = item.get("t")
            if t is None:
                t = last_time
            text = item.get("text", "")
            
            # å¦‚æœæ—¶é—´è·¨åº¦è¶…è¿‡30ç§’æˆ–æ–‡æœ¬å¾ˆé•¿ï¼Œåˆ›å»ºæ–°segment
            if t - current_seg_start > 30 or len(" ".join(current_seg_texts)) > 300:
                if current_seg_texts:
                    seg_text = " ".join(current_seg_texts)
                    # æ¨æ–­attentionå’Œspeech_act
                    attention, speech_act = infer_segment_type(seg_text, clip.get("language", "zh"))
                    
                    segments.append({
                        "id": f"seg_{len(segments)+1:02d}",
                        "start_time": current_seg_start,
                        "end_time": t,
                        "text": seg_text,
                        "attention_focus": attention,
                        "speech_act": speech_act,
                        "trigger": main_trigger if len(segments) == 0 else "prior",
                        "catchphrase": find_catchphrase(seg_text, catchphrases),
                        "emotion_shift": False
                    })
                
                current_seg_texts = [text]
                current_seg_start = t
            else:
                current_seg_texts.append(text)
            
            last_time = t
        
        # å¤„ç†æœ€åä¸€ä¸ªsegment
        if current_seg_texts:
            seg_text = " ".join(current_seg_texts)
            attention, speech_act = infer_segment_type(seg_text, clip.get("language", "zh"))
            
            segments.append({
                "id": f"seg_{len(segments)+1:02d}",
                "start_time": current_seg_start,
                "end_time": clip.get("duration", last_time),
                "text": seg_text,
                "attention_focus": attention,
                "speech_act": speech_act,
                "trigger": main_trigger if len(segments) == 0 else "prior",
                "catchphrase": find_catchphrase(seg_text, catchphrases),
                "emotion_shift": False
            })
        
        annotated.append({
            "clip_id": clip.get("clip_id", ""),
            "source": clip.get("source", ""),
            "language": clip.get("language", "zh"),
            "duration": clip.get("duration", 0),
            "title": clip.get("title", ""),
            "skeleton": skeleton,
            "catchphrases": catchphrases,
            "segments": segments,
            "notes": notes
        })
    
    # ä¿å­˜
    if verbose:
        print(f"\nğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(annotated, f, ensure_ascii=False, indent=2)
    
    if verbose:
        total_segments = sum(len(c["segments"]) for c in annotated)
        print(f"âœ… å®Œæˆï¼å…± {len(annotated)} ä¸ªclips, {total_segments} ä¸ªsegments")
    
    return annotated


def infer_segment_type(text: str, language: str) -> Tuple[str, str]:
    """
    å¯å‘å¼æ¨æ–­segmentçš„attention_focuså’Œspeech_act
    """
    text_lower = text.lower()
    
    # ä¸­æ–‡å…³é”®è¯
    if language == "zh":
        # attention_focus
        if any(kw in text for kw in ["ä½ ä»¬", "å¤§å®¶", "è§‚ä¼—", "æœ‹å‹ä»¬", "chat"]):
            attention = "audience"
        elif any(kw in text for kw in ["è°¢è°¢", "æ„Ÿè°¢", "SC", "æ‰“èµ", "é€çš„"]):
            attention = "specific"
        elif any(kw in text for kw in ["çŒ«", "ç‹—", "å® ç‰©", "ç”»é¢"]):
            attention = "content"
        elif any(kw in text for kw in ["ç›´æ’­", "ä»Šå¤©æ’­", "å¼€æ’­", "ä¸‹æ’­"]):
            attention = "meta"
        else:
            attention = "self"
        
        # speech_act
        if any(kw in text for kw in ["æˆ‘è®°å¾—", "é‚£æ—¶å€™", "å½“æ—¶", "ä»¥å‰", "æœ‰ä¸€æ¬¡"]):
            speech_act = "narrate"
        elif any(kw in text for kw in ["æˆ‘è§‰å¾—", "æˆ‘è®¤ä¸º", "åº”è¯¥", "å»ºè®®"]):
            speech_act = "opine"
        elif any(kw in text for kw in ["ï¼Ÿ", "å—", "å‘¢", "ä»€ä¹ˆ"]) and len(text) < 50:
            speech_act = "elicit"
        elif any(kw in text for kw in ["å¯¹", "æ˜¯çš„", "æ²¡é”™", "ç¡®å®"]):
            speech_act = "respond"
        elif any(kw in text for kw in ["è¯´åˆ°", "è¯è¯´", "ç„¶å", "æ¥ä¸‹æ¥"]):
            speech_act = "pivot"
        else:
            speech_act = "narrate"
    
    # è‹±æ–‡å…³é”®è¯
    else:
        # attention_focus
        if any(kw in text_lower for kw in ["you guys", "chat", "everyone", "y'all"]):
            attention = "audience"
        elif any(kw in text_lower for kw in ["thank", "thanks", "appreciate", "superchat"]):
            attention = "specific"
        elif any(kw in text_lower for kw in ["cat", "dog", "pet", "screen"]):
            attention = "content"
        elif any(kw in text_lower for kw in ["stream", "streaming", "today's"]):
            attention = "meta"
        else:
            attention = "self"
        
        # speech_act
        if any(kw in text_lower for kw in ["i remember", "back then", "when i was", "one time"]):
            speech_act = "narrate"
        elif any(kw in text_lower for kw in ["i think", "i believe", "should", "i feel like"]):
            speech_act = "opine"
        elif "?" in text and len(text) < 100:
            speech_act = "elicit"
        elif any(kw in text_lower for kw in ["yeah", "yes", "exactly", "right"]):
            speech_act = "respond"
        elif any(kw in text_lower for kw in ["anyway", "so", "speaking of", "next"]):
            speech_act = "pivot"
        else:
            speech_act = "narrate"
    
    return attention, speech_act


def find_catchphrase(text: str, catchphrases: List[str]) -> Optional[str]:
    """æŸ¥æ‰¾æ–‡æœ¬ä¸­å‡ºç°çš„å£ç™–"""
    for cp in catchphrases:
        if cp in text:
            return cp
    return None


# ============================================
# å‘½ä»¤è¡Œå…¥å£
# ============================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="VTuberæ•°æ®æ ‡æ³¨Pipeline")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", required=True, help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼ˆä¸è°ƒç”¨LLMï¼‰")
    parser.add_argument("--max-clips", type=int, help="æœ€å¤§å¤„ç†clipæ•°é‡")
    parser.add_argument("--api-key", help="LLM API Key")
    parser.add_argument("--model", default="claude-3-haiku-20240307", help="LLMæ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.quick:
        quick_convert_without_llm(args.input, args.output)
    else:
        client = None
        if args.api_key:
            try:
                import anthropic
                client = anthropic.Anthropic(api_key=args.api_key)
            except ImportError:
                print("éœ€è¦å®‰è£…anthropic: pip install anthropic")
        
        run_annotation_pipeline(
            args.input,
            args.output,
            client=client,
            model=args.model,
            max_clips=args.max_clips
        )
