#!/usr/bin/env python3
"""
echuu-agent æ•°æ®æ ‡æ³¨å…¥å£è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    # å¿«é€Ÿæ ‡æ³¨ï¼ˆä¸è°ƒç”¨LLMï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™ï¼‰
    python run_annotation.py --quick
    
    # LLMç²¾ç»†æ ‡æ³¨
    python run_annotation.py
    
    # æ ‡æ³¨æŒ‡å®šæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    python run_annotation.py --max-clips 5
    
    # æŒ‡å®šè¾“å‡ºè·¯å¾„
    python run_annotation.py -o custom_output.json
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# åŠ è½½ç¯å¢ƒå˜é‡
try:
    from dotenv import load_dotenv
    load_dotenv(PROJECT_ROOT / ".env")
except ImportError:
    print("âš ï¸ python-dotenv æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install python-dotenv")
    print("   å°†ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

# å¯¼å…¥æ ‡æ³¨pipeline
# ç”±äºç›®å½•ååŒ…å«è¿å­—ç¬¦ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
import importlib.util
pipeline_path = Path(__file__).parent / "data_annotation_pipeline.py"
spec = importlib.util.spec_from_file_location("data_annotation_pipeline", pipeline_path)
pipeline_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(pipeline_module)

load_raw_clips = pipeline_module.load_raw_clips
quick_convert_without_llm = pipeline_module.quick_convert_without_llm
run_annotation_pipeline = pipeline_module.run_annotation_pipeline


def get_config():
    """ä»ç¯å¢ƒå˜é‡è·å–é…ç½®"""
    return {
        "anthropic_api_key": os.getenv("ANTHROPIC_API_KEY", ""),
        "openai_api_key": os.getenv("OPENAI_API_KEY", ""),
        "default_model": os.getenv("DEFAULT_MODEL", "claude-sonnet-4-20250514"),
        "raw_data_path": os.getenv("RAW_DATA_PATH", "data/vtuber_raw_clips_for_notebook_full_30_cleaned.jsonl"),
        "annotated_data_path": os.getenv("ANNOTATED_DATA_PATH", "data/annotated_clips.json"),
        "max_concurrent": int(os.getenv("MAX_CONCURRENT_REQUESTS", "3")),
        "request_delay": float(os.getenv("REQUEST_DELAY", "1.0")),
        "verbose": os.getenv("VERBOSE", "true").lower() == "true"
    }


def setup_llm_client(config: dict):
    """æ ¹æ®é…ç½®åˆ›å»ºLLMå®¢æˆ·ç«¯"""
    # ä¼˜å…ˆä½¿ç”¨ Anthropic
    if config["anthropic_api_key"]:
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=config["anthropic_api_key"])
            print("âœ… ä½¿ç”¨ Anthropic Claude API")
            return client, config["default_model"]
        except ImportError:
            print("âš ï¸ anthropic æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install anthropic")
    
    # å…¶æ¬¡ä½¿ç”¨ OpenAI
    if config["openai_api_key"]:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=config["openai_api_key"])
            print("âœ… ä½¿ç”¨ OpenAI API")
            return client, config["default_model"]
        except ImportError:
            print("âš ï¸ openai æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
    
    return None, None


def print_banner():
    """æ‰“å°æ¬¢è¿banner"""
    print()
    print("=" * 60)
    print("  echuu-agent æ•°æ®æ ‡æ³¨å·¥å…·")
    print("=" * 60)
    print()


def print_summary(annotated_clips: list, output_path: str, duration: float):
    """æ‰“å°æ ‡æ³¨ç»“æœæ‘˜è¦"""
    total_segments = sum(len(c.get("segments", [])) for c in annotated_clips)
    zh_clips = sum(1 for c in annotated_clips if c.get("language") == "zh")
    en_clips = len(annotated_clips) - zh_clips
    
    print()
    print("=" * 60)
    print("  æ ‡æ³¨å®Œæˆ!")
    print("=" * 60)
    print(f"  æ€»clipæ•°: {len(annotated_clips)}")
    print(f"    - ä¸­æ–‡: {zh_clips}")
    print(f"    - è‹±æ–‡: {en_clips}")
    print(f"  æ€»segmentæ•°: {total_segments}")
    print(f"  å¹³å‡segments/clip: {total_segments/len(annotated_clips):.1f}")
    print(f"  è€—æ—¶: {duration:.1f}ç§’")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("=" * 60)
    print()


def main():
    parser = argparse.ArgumentParser(
        description="echuu-agent æ•°æ®æ ‡æ³¨å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python run_annotation.py --quick          # å¿«é€Ÿæ¨¡å¼
  python run_annotation.py --max-clips 5    # åªæ ‡æ³¨5ä¸ªclip
  python run_annotation.py -v               # è¯¦ç»†è¾“å‡º
        """
    )
    
    parser.add_argument(
        "-i", "--input",
        help="è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä».envè¯»å–ï¼‰"
    )
    parser.add_argument(
        "-o", "--output",
        help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä».envè¯»å–ï¼‰"
    )
    parser.add_argument(
        "--quick", action="store_true",
        help="å¿«é€Ÿæ¨¡å¼ï¼šä½¿ç”¨å¯å‘å¼è§„åˆ™ï¼Œä¸è°ƒç”¨LLM"
    )
    parser.add_argument(
        "--max-clips", type=int,
        help="æœ€å¤§å¤„ç†clipæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º"
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="åªæ£€æŸ¥é…ç½®ï¼Œä¸æ‰§è¡Œæ ‡æ³¨"
    )
    
    args = parser.parse_args()
    
    # æ‰“å°banner
    print_banner()
    
    # è·å–é…ç½®
    config = get_config()
    
    # å¤„ç†è·¯å¾„
    input_path = args.input or str(PROJECT_ROOT / config["raw_data_path"])
    output_path = args.output or str(PROJECT_ROOT / config["annotated_data_path"])
    verbose = args.verbose or config["verbose"]
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(input_path).exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        sys.exit(1)
    
    # åŠ è½½åŸå§‹æ•°æ®
    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    raw_clips = load_raw_clips(input_path)
    print(f"   æ‰¾åˆ° {len(raw_clips)} ä¸ªclips")
    
    # é™åˆ¶æ•°é‡
    if args.max_clips:
        raw_clips = raw_clips[:args.max_clips]
        print(f"   é™åˆ¶å¤„ç†: {len(raw_clips)} ä¸ªclips")
    
    # Dry run æ¨¡å¼
    if args.dry_run:
        print("\nğŸ” Dry Run æ¨¡å¼ - é…ç½®æ£€æŸ¥")
        print(f"   è¾“å…¥: {input_path}")
        print(f"   è¾“å‡º: {output_path}")
        print(f"   æ¨¡å¼: {'å¿«é€Ÿ' if args.quick else 'LLMæ ‡æ³¨'}")
        if not args.quick:
            if config["anthropic_api_key"]:
                print(f"   API: Anthropic (key: ...{config['anthropic_api_key'][-8:]})")
            elif config["openai_api_key"]:
                print(f"   API: OpenAI (key: ...{config['openai_api_key'][-8:]})")
            else:
                print("   âš ï¸ æœªé…ç½®API Key")
        return
    
    # å¼€å§‹è®¡æ—¶
    start_time = datetime.now()
    
    # æ‰§è¡Œæ ‡æ³¨
    if args.quick:
        print("\nğŸš€ å¿«é€Ÿæ¨¡å¼ï¼ˆå¯å‘å¼è§„åˆ™ï¼‰")
        annotated = quick_convert_without_llm(
            input_path, 
            output_path,
            verbose=verbose
        )
    else:
        # è®¾ç½®LLMå®¢æˆ·ç«¯
        client, model = setup_llm_client(config)
        
        if client is None:
            print("\nâš ï¸ æœªé…ç½®æœ‰æ•ˆçš„API Key")
            print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® ANTHROPIC_API_KEY æˆ– OPENAI_API_KEY")
            print("   æˆ–ä½¿ç”¨ --quick æ¨¡å¼ï¼ˆä¸éœ€è¦APIï¼‰")
            sys.exit(1)
        
        print(f"\nğŸ¤– LLMæ ‡æ³¨æ¨¡å¼ (æ¨¡å‹: {model})")
        annotated = run_annotation_pipeline(
            input_path,
            output_path,
            client=client,
            model=model,
            max_clips=args.max_clips,
            verbose=verbose
        )
    
    # è®¡ç®—è€—æ—¶
    duration = (datetime.now() - start_time).total_seconds()
    
    # æ‰“å°æ‘˜è¦
    print_summary(annotated, output_path, duration)
    
    print("âœ¨ ä¸‹ä¸€æ­¥: åœ¨ echuu notebook ä¸­åŠ è½½ annotated_clips.json")


if __name__ == "__main__":
    main()
