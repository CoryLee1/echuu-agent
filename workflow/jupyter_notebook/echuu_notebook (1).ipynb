{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# echuu - AI VTuber Performance Engine\n",
    "\n",
    "## æ¶æ„æ¦‚è§ˆ\n",
    "\n",
    "```\n",
    "ç”¨æˆ·è¾“å…¥(OCè®¾å®š + è¯é¢˜)\n",
    "         â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              Single Performer Agent                         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Phase 1: Script Generation (Pre-show)                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  Input: Persona + Background + Topic                â”‚   â”‚\n",
    "â”‚  â”‚  Output: 4-6ä¸ªNarrativeNode (Hookâ†’Climaxâ†’Resolution)â”‚   â”‚\n",
    "â”‚  â”‚  æ¯ä¸ªèŠ‚ç‚¹æœ‰ interruption_cost (0.0-1.0)             â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                          â†“                                  â”‚\n",
    "â”‚  Phase 2: Cognitive Loop (Live)                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  æ¯10ç§’æ‰§è¡Œä¸€æ¬¡:                                     â”‚   â”‚\n",
    "â”‚  â”‚  1. æ„ŸçŸ¥: å‰§æœ¬è¿›åº¦ + å¼¹å¹•é˜Ÿåˆ— + æ—¶é—´                â”‚   â”‚\n",
    "â”‚  â”‚  2. å†³ç­–: urgency - cost = å›åº” or ç»§ç»­?           â”‚   â”‚\n",
    "â”‚  â”‚  3. è¾“å‡º: inner_monologue + speech + emotion        â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†“\n",
    "è¾“å‡º: é¢„ç½®è„šæœ¬JSON + å®æ—¶è¡¨æ¼”æ–‡æœ¬ï¼ˆå«å¯è§çš„å†…å¿ƒç‹¬ç™½ï¼‰\n",
    "```\n",
    "\n",
    "## æ ¸å¿ƒåˆ›æ–°\n",
    "\n",
    "1. **Interruption Cost** - åŠ¨æ€å†³å®šæ˜¯å¦å›åº”å¼¹å¹•\n",
    "2. **Inner Monologueå¯è§** - è®©è§‚ä¼—çœ‹åˆ°AIçš„\"æ€è€ƒè¿‡ç¨‹\"\n",
    "3. **æ•°æ®é©±åŠ¨** - ä»çœŸå®ä¸»æ’­åˆ‡ç‰‡å­¦ä¹ è¡Œä¸ºæ¨¡å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: ç¯å¢ƒå‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install openai anthropic -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from enum import Enum\n",
    "from collections import defaultdict, Counter\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# è®¾ç½®API Keyï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-key-here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: æ•°æ®ç»“æ„å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFocus(str, Enum):\n",
    "    \"\"\"æ³¨æ„åŠ›ç„¦ç‚¹\"\"\"\n",
    "    SELF = \"self\"           # è®²è‡ªå·±çš„äº‹\n",
    "    AUDIENCE = \"audience\"   # å’Œè§‚ä¼—äº’åŠ¨\n",
    "    CONTENT = \"content\"     # è®²å†…å®¹/çŸ¥è¯†\n",
    "    SPECIFIC = \"specific\"   # å¯¹ç‰¹å®šäºº/ç‰©\n",
    "    META = \"meta\"           # å…³äºç›´æ’­æœ¬èº«\n",
    "\n",
    "\n",
    "class SpeechAct(str, Enum):\n",
    "    \"\"\"è¨€è¯­è¡Œä¸º\"\"\"\n",
    "    NARRATE = \"narrate\"     # å™è¿°æ•…äº‹\n",
    "    OPINE = \"opine\"         # å‘è¡¨è§‚ç‚¹\n",
    "    RESPOND = \"respond\"     # å›åº”å¼¹å¹•\n",
    "    ELICIT = \"elicit\"       # å¼•å¯¼äº’åŠ¨\n",
    "    PIVOT = \"pivot\"         # è½¬åœº\n",
    "\n",
    "\n",
    "class Decision(str, Enum):\n",
    "    \"\"\"è®¤çŸ¥å¾ªç¯å†³ç­–\"\"\"\n",
    "    CONTINUE = \"continue\"   # ç»§ç»­å‰§æœ¬\n",
    "    RESPOND = \"respond\"     # å›åº”å¼¹å¹•\n",
    "    IMPROVISE = \"improvise\" # è·‘é¢˜å³å…´\n",
    "    SILENCE = \"silence\"     # æ²‰é»˜/æ€è€ƒ\n",
    "    REACT = \"react\"         # éè¯­è¨€ååº”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NarrativeNode:\n",
    "    \"\"\"å‰§æœ¬ä¸­çš„ä¸€ä¸ªå™äº‹èŠ‚ç‚¹\"\"\"\n",
    "    stage: str                      # \"Hook\", \"Build-up\", \"Climax\", \"Resolution\"\n",
    "    goal: str                       # è¿™ä¸ªé˜¶æ®µè¦è¾¾æˆçš„ç›®æ ‡\n",
    "    target_attention: str           # ç›®æ ‡attention focus\n",
    "    target_speech_act: str          # ç›®æ ‡speech act\n",
    "    duration_sec: int               # é¢„è®¡æ—¶é•¿\n",
    "    interruption_cost: float        # 0.0-1.0ï¼Œè¢«æ‰“æ–­çš„ä»£ä»·\n",
    "    content_hint: str = \"\"          # å†…å®¹æç¤º\n",
    "    \n",
    "    def get_current_cost(self, elapsed_ratio: float) -> float:\n",
    "        \"\"\"\n",
    "        éšç€èŠ‚ç‚¹è¿›åº¦æ¨è¿›ï¼Œcostä¼šé™ä½ï¼ˆå¿«è®²å®Œäº†å°±éšæ„äº†ï¼‰\n",
    "        \n",
    "        è¿™æ˜¯ä¸€ä¸ªå…³é”®æœºåˆ¶ï¼š\n",
    "        - elapsed_ratio < 50%: ç»´æŒåŸcost\n",
    "        - elapsed_ratio > 50%: cost * 0.8\n",
    "        - elapsed_ratio > 80%: cost * 0.5 (å¿«ç»“æŸäº†ï¼Œå¯ä»¥è½¬åœº)\n",
    "        \"\"\"\n",
    "        if elapsed_ratio > 0.8:\n",
    "            return self.interruption_cost * 0.5\n",
    "        elif elapsed_ratio > 0.5:\n",
    "            return self.interruption_cost * 0.8\n",
    "        return self.interruption_cost\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Danmaku:\n",
    "    \"\"\"å¼¹å¹•\"\"\"\n",
    "    text: str\n",
    "    urgency: float = 0.3        # 0.0-1.0\n",
    "    is_sc: bool = False         # æ˜¯å¦æ˜¯SC/æ‰“èµ\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text(cls, text: str) -> \"Danmaku\":\n",
    "        \"\"\"ä»æ–‡æœ¬è‡ªåŠ¨åˆ¤æ–­urgency\"\"\"\n",
    "        urgency = 0.3\n",
    "        is_sc = False\n",
    "        \n",
    "        if any(kw in text for kw in [\"SC\", \"Â¥\", \"$\", \"æ‰“èµ\", \"ç¤¼ç‰©\"]):\n",
    "            urgency = 0.9\n",
    "            is_sc = True\n",
    "        elif \"?\" in text or \"ï¼Ÿ\" in text:\n",
    "            urgency = 0.5\n",
    "        elif any(kw in text for kw in [\"ç”Ÿæ—¥\", \"ç¬¬ä¸€æ¬¡\", \"æ±‚\", \"å¸®\", \"æ€ä¹ˆåŠ\"]):\n",
    "            urgency = 0.6\n",
    "            \n",
    "        return cls(text=text, urgency=urgency, is_sc=is_sc)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PerformanceState:\n",
    "    \"\"\"è¡¨æ¼”çŠ¶æ€\"\"\"\n",
    "    # OCè®¾å®š\n",
    "    name: str\n",
    "    persona: str\n",
    "    background: str\n",
    "    topic: str\n",
    "    \n",
    "    # å‰§æœ¬\n",
    "    script: List[NarrativeNode] = field(default_factory=list)\n",
    "    current_node_idx: int = 0\n",
    "    node_elapsed_sec: float = 0.0\n",
    "    \n",
    "    # å¼¹å¹•é˜Ÿåˆ—\n",
    "    danmaku_queue: List[Danmaku] = field(default_factory=list)\n",
    "    ignored_count: int = 0  # è¿ç»­å¿½ç•¥çš„å¼¹å¹•æ•°\n",
    "    \n",
    "    # è¡¨æ¼”è®°å½•\n",
    "    performance_log: List[Dict] = field(default_factory=list)\n",
    "    total_elapsed_sec: float = 0.0\n",
    "    \n",
    "    # ä»æ•°æ®æå–çš„å‚è€ƒ\n",
    "    catchphrases: List[str] = field(default_factory=list)\n",
    "    example_hooks: List[str] = field(default_factory=list)\n",
    "    example_punchlines: List[str] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Pattern Analyzer - ä»æ•°æ®å­¦ä¹ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternAnalyzer:\n",
    "    \"\"\"ä»æ ‡æ³¨æ•°æ®ä¸­æå–æ¨¡å¼\"\"\"\n",
    "    \n",
    "    def __init__(self, annotated_clips: List[Dict]):\n",
    "        self.clips = annotated_clips\n",
    "        self.all_segments = []\n",
    "        for clip in annotated_clips:\n",
    "            self.all_segments.extend(clip.get(\"segments\", []))\n",
    "    \n",
    "    def compute_attention_transitions(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"è®¡ç®—attentionè½¬ç§»æ¦‚ç‡\"\"\"\n",
    "        trans = defaultdict(lambda: defaultdict(int))\n",
    "        for clip in self.clips:\n",
    "            segs = clip.get(\"segments\", [])\n",
    "            for i in range(len(segs) - 1):\n",
    "                f = segs[i].get(\"attention_focus\", \"self\")\n",
    "                t = segs[i+1].get(\"attention_focus\", \"self\")\n",
    "                trans[f][t] += 1\n",
    "        \n",
    "        prob = {}\n",
    "        for f, tos in trans.items():\n",
    "            total = sum(tos.values())\n",
    "            prob[f] = {t: c/total for t, c in tos.items()}\n",
    "        return prob\n",
    "    \n",
    "    def infer_baseline_costs(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        æ¨æ–­ä¸åŒattentionä¸‹è¢«æ‰“æ–­çš„ä»£ä»·ï¼ˆå¿½ç•¥å¼¹å¹•çš„æ¯”ä¾‹ï¼‰\n",
    "        \n",
    "        æ ¸å¿ƒé€»è¾‘ï¼š\n",
    "        - å¦‚æœtriggeræ˜¯danmakuä½†speech_actä¸æ˜¯respond\n",
    "        - è¯´æ˜ä¸»æ’­é€‰æ‹©å¿½ç•¥äº†è¿™æ¡å¼¹å¹•\n",
    "        - å¿½ç•¥ç‡è¶Šé«˜ = interruption costè¶Šé«˜\n",
    "        \"\"\"\n",
    "        focus_stats = defaultdict(lambda: {\"total\": 0, \"ignored\": 0})\n",
    "        \n",
    "        for seg in self.all_segments:\n",
    "            focus = seg.get(\"attention_focus\", \"self\")\n",
    "            trigger = seg.get(\"trigger\", \"self\")\n",
    "            act = seg.get(\"speech_act\", \"narrate\")\n",
    "            \n",
    "            if trigger == \"danmaku\":\n",
    "                focus_stats[focus][\"total\"] += 1\n",
    "                if act != \"respond\":\n",
    "                    focus_stats[focus][\"ignored\"] += 1\n",
    "        \n",
    "        costs = {}\n",
    "        for focus, stats in focus_stats.items():\n",
    "            if stats[\"total\"] > 0:\n",
    "                costs[focus] = stats[\"ignored\"] / stats[\"total\"]\n",
    "            else:\n",
    "                costs[focus] = 0.5  # é»˜è®¤\n",
    "        return costs\n",
    "    \n",
    "    def extract_skeletons(self) -> List[Tuple[str, int]]:\n",
    "        \"\"\"æå–å™äº‹éª¨æ¶\"\"\"\n",
    "        skeletons = [c.get(\"skeleton\", \"\") for c in self.clips if c.get(\"skeleton\")]\n",
    "        return Counter(skeletons).most_common(10)\n",
    "    \n",
    "    def extract_catchphrases(self, language: str = None) -> List[Tuple[str, int]]:\n",
    "        \"\"\"æå–å£ç™–\"\"\"\n",
    "        cps = []\n",
    "        for c in self.clips:\n",
    "            if language and c.get(\"language\") != language:\n",
    "                continue\n",
    "            cps.extend(c.get(\"catchphrases\", []))\n",
    "        return Counter(cps).most_common(20)\n",
    "    \n",
    "    def extract_hooks(self, language: str = None) -> List[str]:\n",
    "        \"\"\"æå–å¼€åœºç¤ºä¾‹\"\"\"\n",
    "        hooks = []\n",
    "        for c in self.clips:\n",
    "            if language and c.get(\"language\") != language:\n",
    "                continue\n",
    "            segs = c.get(\"segments\", [])\n",
    "            if segs:\n",
    "                hooks.append(segs[0].get(\"text\", \"\")[:100])\n",
    "        return hooks[:10]\n",
    "    \n",
    "    def extract_punchlines(self, language: str = None) -> List[str]:\n",
    "        \"\"\"æå–æ”¶å°¾ç¤ºä¾‹\"\"\"\n",
    "        punches = []\n",
    "        for c in self.clips:\n",
    "            if language and c.get(\"language\") != language:\n",
    "                continue\n",
    "            segs = c.get(\"segments\", [])\n",
    "            if segs:\n",
    "                punches.append(segs[-1].get(\"text\", \"\")[:100])\n",
    "        return punches[:10]\n",
    "    \n",
    "    def get_report(self) -> str:\n",
    "        \"\"\"ç”Ÿæˆåˆ†ææŠ¥å‘Š\"\"\"\n",
    "        lines = [\n",
    "            \"=\" * 50,\n",
    "            \"ğŸ“Š Pattern Analysis Report\",\n",
    "            \"=\" * 50,\n",
    "            f\"Total clips: {len(self.clips)}, Total segments: {len(self.all_segments)}\",\n",
    "        ]\n",
    "        \n",
    "        lines.append(\"\\n### Attention Transitions\")\n",
    "        for f, tos in self.compute_attention_transitions().items():\n",
    "            top = sorted(tos.items(), key=lambda x: -x[1])[:3]\n",
    "            lines.append(f\"- `{f}` â†’ \" + \", \".join(f\"`{t}`:{p:.0%}\" for t, p in top))\n",
    "        \n",
    "        lines.append(\"\\n### Inferred Interruption Costs\")\n",
    "        for focus, cost in self.infer_baseline_costs().items():\n",
    "            bar = \"â–ˆ\" * int(cost * 10) + \"â–‘\" * (10 - int(cost * 10))\n",
    "            lines.append(f\"- `{focus}`: {bar} {cost:.2f}\")\n",
    "        \n",
    "        lines.append(\"\\n### Top Catchphrases\")\n",
    "        cps = self.extract_catchphrases()[:10]\n",
    "        lines.append(\", \".join(f'\"{c}\"({n})' for c, n in cps))\n",
    "        \n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: åŠ è½½æ•°æ®å¹¶åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ ‡æ³¨æ•°æ®\n",
    "DATA_PATH = \"/mnt/user-data/uploads/annotated_clips.json\"\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    annotated_clips = json.load(f)\n",
    "\n",
    "print(f\"âœ… åŠ è½½äº† {len(annotated_clips)} ä¸ªclips\")\n",
    "\n",
    "# åˆ›å»ºåˆ†æå™¨\n",
    "analyzer = PatternAnalyzer(annotated_clips)\n",
    "\n",
    "# æ˜¾ç¤ºæŠ¥å‘Š\n",
    "display(Markdown(analyzer.get_report()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹ä¸€äº›ç¤ºä¾‹æ•°æ®\n",
    "print(\"\\nğŸ“ Hookç¤ºä¾‹ (ä¸­æ–‡):\")\n",
    "for i, hook in enumerate(analyzer.extract_hooks(\"zh\")[:3], 1):\n",
    "    print(f\"  {i}. {hook[:60]}...\")\n",
    "\n",
    "print(\"\\nğŸ“ Punchlineç¤ºä¾‹ (ä¸­æ–‡):\")\n",
    "for i, punch in enumerate(analyzer.extract_punchlines(\"zh\")[:3], 1):\n",
    "    print(f\"  {i}. {punch[:60]}...\")\n",
    "\n",
    "print(\"\\nğŸ“ å™äº‹éª¨æ¶ç¤ºä¾‹:\")\n",
    "for skel, count in analyzer.extract_skeletons()[:3]:\n",
    "    print(f\"  [{count}æ¬¡] {skel[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    \"\"\"LLMè°ƒç”¨å°è£… - æ”¯æŒçœŸå®APIæˆ–Mockæ¨¡å¼\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        self.client = None\n",
    "        \n",
    "        if self.api_key:\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                self.client = OpenAI(\n",
    "                    api_key=self.api_key,\n",
    "                    base_url=\"https://api.anthropic.com/v1/\"\n",
    "                )\n",
    "                print(\"âœ… LLM Client initialized with API\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ API init failed: {e}, using mock mode\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No API key, using mock mode\")\n",
    "    \n",
    "    def call(self, prompt: str, system: str = None, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"è°ƒç”¨LLM\"\"\"\n",
    "        if self.client:\n",
    "            try:\n",
    "                messages = []\n",
    "                if system:\n",
    "                    messages.append({\"role\": \"system\", \"content\": system})\n",
    "                messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "                \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"claude-sonnet-4-20250514\",\n",
    "                    max_tokens=max_tokens,\n",
    "                    messages=messages\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                print(f\"LLM Error: {e}\")\n",
    "        \n",
    "        return self._mock_response(prompt)\n",
    "    \n",
    "    def _mock_response(self, prompt: str) -> str:\n",
    "        \"\"\"Mockå“åº” - ç”¨äºæ¼”ç¤º\"\"\"\n",
    "        if \"ç”Ÿæˆå‰§æœ¬\" in prompt or \"script\" in prompt.lower() or \"JSONæ•°ç»„\" in prompt:\n",
    "            return \"\"\"[\n",
    "{\"stage\": \"Hook\", \"goal\": \"ç”¨æ‚¬å¿µå¼€åœºå¸å¼•æ³¨æ„\", \"attention\": \"audience\", \"speech_act\": \"elicit\", \"duration\": 20, \"cost\": 0.3, \"hint\": \"æå‡ºä¸€ä¸ªå¼•äººå¥½å¥‡çš„é—®é¢˜\"},\n",
    "{\"stage\": \"Build-up\", \"goal\": \"é“ºå«èƒŒæ™¯å’Œæƒ…ç»ª\", \"attention\": \"self\", \"speech_act\": \"narrate\", \"duration\": 40, \"cost\": 0.6, \"hint\": \"æè¿°å½“æ—¶çš„å¤„å¢ƒ\"},\n",
    "{\"stage\": \"Climax\", \"goal\": \"è®²è¿°æœ€å…³é”®çš„è½¬æŠ˜\", \"attention\": \"self\", \"speech_act\": \"narrate\", \"duration\": 40, \"cost\": 0.9, \"hint\": \"æƒ…æ„Ÿçˆ†å‘ç‚¹\"},\n",
    "{\"stage\": \"Resolution\", \"goal\": \"æ€»ç»“æ„Ÿæ‚Ÿ\", \"attention\": \"audience\", \"speech_act\": \"opine\", \"duration\": 20, \"cost\": 0.4, \"hint\": \"å’Œè§‚ä¼—åˆ†äº«å¿ƒå¾—\"}\n",
    "]\"\"\"\n",
    "        else:\n",
    "            # æ ¹æ®contextåŠ¨æ€ç”Ÿæˆmock response\n",
    "            return self._dynamic_mock(prompt)\n",
    "    \n",
    "    def _dynamic_mock(self, prompt: str) -> str:\n",
    "        \"\"\"åŠ¨æ€Mock - æ¨¡æ‹Ÿè®¤çŸ¥å¾ªç¯\"\"\"\n",
    "        import random\n",
    "        \n",
    "        # è§£æpromptä¸­çš„å…³é”®ä¿¡æ¯\n",
    "        cost = 0.5\n",
    "        urgency = 0.3\n",
    "        stage = \"Unknown\"\n",
    "        \n",
    "        if \"cost:\" in prompt.lower():\n",
    "            try:\n",
    "                cost = float(prompt.split(\"cost:\")[1].split()[0])\n",
    "            except: pass\n",
    "        if \"urgency\" in prompt.lower() and \"æœ€é«˜ç´§æ€¥åº¦\" in prompt:\n",
    "            try:\n",
    "                urgency = float(prompt.split(\"æœ€é«˜ç´§æ€¥åº¦:\")[1].split()[0])\n",
    "            except: pass\n",
    "        if \"å½“å‰é˜¶æ®µ:\" in prompt:\n",
    "            stage = prompt.split(\"å½“å‰é˜¶æ®µ:\")[1].split()[0]\n",
    "        \n",
    "        # å†³ç­–é€»è¾‘\n",
    "        delta = urgency - cost\n",
    "        \n",
    "        monologues = {\n",
    "            \"Hook\": [\"å…ˆæŠŠæ•…äº‹èµ·ä¸ªå¤´...\", \"çœ‹çœ‹æ€ä¹ˆå¸å¼•å¤§å®¶æ³¨æ„\", \"å¼€åœºè¦æŠ“äºº\"],\n",
    "            \"Build-up\": [\"æ…¢æ…¢é“ºå«ä¸€ä¸‹...\", \"è®©å¤§å®¶äº†è§£èƒŒæ™¯\", \"æƒ…ç»ªè¦é…é…¿èµ·æ¥\"],\n",
    "            \"Climax\": [\"è¿™æ®µå¾ˆé‡è¦ï¼Œä¸“æ³¨è®²...\", \"å…³é”®æ—¶åˆ»åˆ°äº†\", \"ä¸æƒ³è¢«æ‰“æ–­...\"],\n",
    "            \"Resolution\": [\"è¯¥æ€»ç»“äº†~\", \"å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹æ„Ÿå—\", \"æ”¶ä¸ªå°¾å§\"]\n",
    "        }\n",
    "        \n",
    "        speeches = {\n",
    "            \"Hook\": [\"è¯´èµ·è¿™ä¸ªäº‹å„¿å•Š...\", \"ä½ ä»¬çŸ¥é“å—...\", \"æˆ‘è·Ÿä½ ä»¬è®²...\"],\n",
    "            \"Build-up\": [\"é‚£æ—¶å€™æˆ‘...\", \"å½“æ—¶çš„æƒ…å†µæ˜¯è¿™æ ·çš„...\", \"ç„¶åå‘¢...\"],\n",
    "            \"Climax\": [\"ç»“æœä½ çŒœæ€ä¹ˆç€...\", \"æœ€ç¦»è°±çš„æ˜¯...\", \"ç„¶åæˆ‘å°±...\"],\n",
    "            \"Resolution\": [\"æ‰€ä»¥è¯´...\", \"æˆ‘è§‰å¾—å§...\", \"åæ­£å°±æ˜¯...\"]\n",
    "        }\n",
    "        \n",
    "        if delta > 0.3:\n",
    "            decision = \"respond\"\n",
    "            if urgency > 0.8:\n",
    "                monologue = \"æœ‰SCï¼å¾—æ„Ÿè°¢ä¸€ä¸‹\"\n",
    "            else:\n",
    "                monologue = \"è¿™æ¡å¼¹å¹•æŒºæœ‰æ„æ€\"\n",
    "            speech = \"è¯¶ï¼Ÿæœ‰äººé—®è¿™ä¸ª...\"\n",
    "            emotion = \"engaged\"\n",
    "        elif cost > 0.8:\n",
    "            decision = \"continue\"\n",
    "            if urgency > 0.7:\n",
    "                monologue = \"çœ‹åˆ°å¼¹å¹•äº†ï¼Œä½†è¿™æ®µå¤ªé‡è¦ï¼Œç­‰ä¼šå„¿å†è¯´\"\n",
    "            else:\n",
    "                monologue = random.choice(monologues.get(stage, [\"ç»§ç»­...\"]))\n",
    "            speech = random.choice(speeches.get(stage, [\"...\"]))\n",
    "            emotion = \"focused\"\n",
    "        else:\n",
    "            decision = \"continue\"\n",
    "            monologue = random.choice(monologues.get(stage, [\"ç»§ç»­...\"]))\n",
    "            speech = random.choice(speeches.get(stage, [\"...\"]))\n",
    "            emotion = \"neutral\"\n",
    "        \n",
    "        return json.dumps({\n",
    "            \"inner_monologue\": monologue,\n",
    "            \"decision\": decision,\n",
    "            \"speech\": speech,\n",
    "            \"emotion\": emotion\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–LLM Client\n",
    "llm = LLMClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Script Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptGenerator:\n",
    "    \"\"\"å‰§æœ¬éª¨æ¶ç”Ÿæˆå™¨\"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„VTuberå‰§æœ¬ç¼–å‰§ã€‚ä½ éœ€è¦æ ¹æ®è§’è‰²è®¾å®šå’Œç›´æ’­ä¸»é¢˜ï¼Œè®¾è®¡ä¸€ä¸ª2åˆ†é’Ÿçš„è¡¨æ¼”å‰§æœ¬éª¨æ¶ã€‚\n",
    "\n",
    "ä½ ç†Ÿæ‚‰çœŸäººä¸»æ’­çš„å™äº‹æ¨¡å¼ï¼š\n",
    "- Hook: ç”¨é—®é¢˜/æ‚¬å¿µ/å…±é¸£ç‚¹å¼€åœº\n",
    "- Build-up: é“ºå«èƒŒæ™¯ã€ç§¯ç´¯æƒ…ç»ª\n",
    "- Climax: æƒ…æ„Ÿçˆ†å‘ç‚¹ã€å…³é”®ä¿¡æ¯\n",
    "- Resolution: æ€»ç»“å‡åã€å’Œè§‚ä¼—è¿æ¥\n",
    "\n",
    "ä½ éœ€è¦è€ƒè™‘interruption_costï¼ˆè¢«å¼¹å¹•æ‰“æ–­çš„ä»£ä»·ï¼‰ï¼š\n",
    "- 0.0-0.3: å¯ä»¥éšæ—¶æ‰“æ–­ï¼ˆé—²èŠã€å¼€åœºï¼‰\n",
    "- 0.4-0.6: æœ€å¥½ä¸æ‰“æ–­ä½†å¯ä»¥ï¼ˆé“ºå«é˜¶æ®µï¼‰\n",
    "- 0.7-0.9: å°½é‡ä¸æ‰“æ–­ï¼ˆé«˜æ½®é˜¶æ®µï¼‰\n",
    "- 0.9+: ç»å¯¹ä¸èƒ½æ‰“æ–­ï¼ˆæƒ…æ„Ÿçˆ†å‘ï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self, llm: LLMClient, analyzer: PatternAnalyzer = None):\n",
    "        self.llm = llm\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def generate(self, name: str, persona: str, background: str, \n",
    "                 topic: str, language: str = \"zh\") -> List[NarrativeNode]:\n",
    "        \"\"\"ç”Ÿæˆå‰§æœ¬éª¨æ¶\"\"\"\n",
    "        \n",
    "        # è·å–å‚è€ƒæ•°æ®\n",
    "        example_skeletons = \"\"\n",
    "        example_hooks = \"\"\n",
    "        catchphrases = \"\"\n",
    "        \n",
    "        if self.analyzer:\n",
    "            skels = self.analyzer.extract_skeletons()[:3]\n",
    "            example_skeletons = \"\\n\".join(f\"- {s}\" for s, _ in skels)\n",
    "            \n",
    "            hooks = self.analyzer.extract_hooks(language)[:3]\n",
    "            example_hooks = \"\\n\".join(f'- \"{h[:50]}...\"' for h in hooks)\n",
    "            \n",
    "            cps = self.analyzer.extract_catchphrases(language)[:5]\n",
    "            catchphrases = \", \".join(f'\"{c}\"' for c, _ in cps)\n",
    "        \n",
    "        prompt = f\"\"\"è¯·ä¸ºä»¥ä¸‹VTuberè®¾è®¡ä¸€ä¸ª2åˆ†é’Ÿçš„ç›´æ’­å‰§æœ¬éª¨æ¶ï¼š\n",
    "\n",
    "## è§’è‰²ä¿¡æ¯\n",
    "- åå­—: {name}\n",
    "- äººè®¾: {persona}\n",
    "- èƒŒæ™¯: {background}\n",
    "- ä»Šæ—¥è¯é¢˜: {topic}\n",
    "\n",
    "## å‚è€ƒï¼šçœŸäººä¸»æ’­çš„å™äº‹éª¨æ¶\n",
    "{example_skeletons or \"ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰\"}\n",
    "\n",
    "## å‚è€ƒï¼šå¼€åœºHookç¤ºä¾‹\n",
    "{example_hooks or \"ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰\"}\n",
    "\n",
    "## å¯é€‰å£ç™–\n",
    "{catchphrases or \"ï¼ˆæ— å‚è€ƒæ•°æ®ï¼‰\"}\n",
    "\n",
    "## è¾“å‡ºè¦æ±‚\n",
    "è¾“å‡ºä¸€ä¸ªJSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€ä¸ªå™äº‹èŠ‚ç‚¹ã€‚\n",
    "è¯·è®¾è®¡4-6ä¸ªèŠ‚ç‚¹ï¼Œæ€»æ—¶é•¿çº¦120ç§’ã€‚ç›´æ¥è¾“å‡ºJSONã€‚\"\"\"\n",
    "\n",
    "        response = self.llm.call(prompt, system=self.SYSTEM_PROMPT)\n",
    "        \n",
    "        # è§£æå“åº”\n",
    "        try:\n",
    "            if \"```\" in response:\n",
    "                json_str = response.split(\"```\")[1]\n",
    "                if json_str.startswith(\"json\"):\n",
    "                    json_str = json_str[4:]\n",
    "            else:\n",
    "                json_str = response\n",
    "            \n",
    "            nodes_data = json.loads(json_str.strip())\n",
    "            \n",
    "            nodes = []\n",
    "            for n in nodes_data:\n",
    "                node = NarrativeNode(\n",
    "                    stage=n.get(\"stage\", \"Unknown\"),\n",
    "                    goal=n.get(\"goal\", \"\"),\n",
    "                    target_attention=n.get(\"attention\", \"self\"),\n",
    "                    target_speech_act=n.get(\"speech_act\", \"narrate\"),\n",
    "                    duration_sec=n.get(\"duration\", 30),\n",
    "                    interruption_cost=n.get(\"cost\", 0.5),\n",
    "                    content_hint=n.get(\"hint\", \"\")\n",
    "                )\n",
    "                nodes.append(node)\n",
    "            return nodes\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Parse error: {e}\")\n",
    "            return self._fallback_script(topic)\n",
    "    \n",
    "    def _fallback_script(self, topic: str) -> List[NarrativeNode]:\n",
    "        \"\"\"è§£æå¤±è´¥æ—¶çš„åå¤‡å‰§æœ¬\"\"\"\n",
    "        return [\n",
    "            NarrativeNode(\"Hook\", f\"å¼•å‡º{topic}\", \"audience\", \"elicit\", 20, 0.3),\n",
    "            NarrativeNode(\"Build-up\", \"é“ºå«èƒŒæ™¯\", \"self\", \"narrate\", 40, 0.6),\n",
    "            NarrativeNode(\"Climax\", \"æ ¸å¿ƒå†…å®¹\", \"self\", \"narrate\", 40, 0.9),\n",
    "            NarrativeNode(\"Resolution\", \"æ€»ç»“å‡å\", \"audience\", \"opine\", 20, 0.4),\n",
    "        ]\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "script_gen = ScriptGenerator(llm, analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cognitive Performer - æ ¸å¿ƒï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CognitivePerformer:\n",
    "    \"\"\"è®¤çŸ¥è¡¨æ¼”è€… - å®æ—¶å†³ç­–å’Œç”Ÿæˆ\"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"ä½ æ˜¯ä¸€ä¸ªVTuberçš„\"å†…å¿ƒ\"ã€‚ä½ éœ€è¦åœ¨æ¯ä¸ªæ—¶åˆ»ï¼š\n",
    "1. æ„ŸçŸ¥å½“å‰çŠ¶æ€ï¼ˆå‰§æœ¬è¿›åº¦ã€å¼¹å¹•ã€æ—¶é—´ï¼‰\n",
    "2. åšå‡ºå†³ç­–ï¼ˆç»§ç»­å‰§æœ¬/å›åº”å¼¹å¹•/å³å…´/æ²‰é»˜ï¼‰\n",
    "3. è¾“å‡ºå†…å¿ƒç‹¬ç™½ï¼ˆè¿™ä¼šæ˜¾ç¤ºç»™è§‚ä¼—ï¼Œæ˜¯killer featureï¼‰\n",
    "4. ç”Ÿæˆå°è¯\n",
    "\n",
    "å…³é”®ï¼šå†…å¿ƒç‹¬ç™½è¦å±•ç¤ºä½ çš„\"æ€è€ƒè¿‡ç¨‹\"ï¼Œè®©è§‚ä¼—æ„Ÿå—åˆ°AIçš„agencyã€‚\n",
    "- ä¸æ˜¯å‡è£…åƒäººï¼Œè€Œæ˜¯è®©è§‚ä¼—çœ‹åˆ°ä½ åœ¨\"æ€è€ƒ\"å’Œ\"é€‰æ‹©\"\n",
    "- \"é€‰æ‹©ä¸å›åº”\"ä¹Ÿæ˜¯ä¸€ç§è¡¨è¾¾\n",
    "- å†…å¿ƒç‹¬ç™½è¦ç®€çŸ­ã€çœŸå®ã€æœ‰personality\"\"\"\n",
    "\n",
    "    def __init__(self, llm: LLMClient, analyzer: PatternAnalyzer = None):\n",
    "        self.llm = llm\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def step(self, state: PerformanceState, new_danmaku: List[Danmaku] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        æ‰§è¡Œä¸€æ¬¡è®¤çŸ¥å¾ªç¯\n",
    "        \n",
    "        æ ¸å¿ƒå…¬å¼: decision_value = urgency - cost\n",
    "        - æ­£æ•° â†’ å€¾å‘å›åº”å¼¹å¹•\n",
    "        - è´Ÿæ•° â†’ å€¾å‘ç»§ç»­å‰§æœ¬\n",
    "        \"\"\"\n",
    "        \n",
    "        if new_danmaku:\n",
    "            state.danmaku_queue.extend(new_danmaku)\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦ç»“æŸ\n",
    "        if state.current_node_idx >= len(state.script):\n",
    "            return self._generate_ending(state)\n",
    "        \n",
    "        current_node = state.script[state.current_node_idx]\n",
    "        elapsed_ratio = state.node_elapsed_sec / current_node.duration_sec if current_node.duration_sec > 0 else 1.0\n",
    "        current_cost = current_node.get_current_cost(elapsed_ratio)\n",
    "        \n",
    "        # è¯„ä¼°å¼¹å¹•\n",
    "        max_urgency = 0.0\n",
    "        most_urgent = None\n",
    "        for d in state.danmaku_queue:\n",
    "            if d.urgency > max_urgency:\n",
    "                max_urgency = d.urgency\n",
    "                most_urgent = d\n",
    "        \n",
    "        # æ„å»ºprompt\n",
    "        recent_danmaku = [d.text for d in state.danmaku_queue[-5:]]\n",
    "        \n",
    "        prompt = f\"\"\"## å½“å‰çŠ¶æ€\n",
    "\n",
    "### è§’è‰²\n",
    "åå­—: {state.name}\n",
    "äººè®¾: {state.persona}\n",
    "\n",
    "### å‰§æœ¬è¿›åº¦\n",
    "å½“å‰é˜¶æ®µ: {current_node.stage}\n",
    "é˜¶æ®µç›®æ ‡: {current_node.goal}\n",
    "é˜¶æ®µè¿›åº¦: {elapsed_ratio:.0%}\n",
    "å†…å®¹æç¤º: {current_node.content_hint}\n",
    "\n",
    "### å™äº‹ä»£ä»·\n",
    "å½“å‰cost: {current_cost:.2f} (0=éšæ„æ‰“æ–­, 1=ç»å¯¹ä¸èƒ½æ‰“æ–­)\n",
    "\n",
    "### å¼¹å¹•æƒ…å†µ\n",
    "æœ€è¿‘å¼¹å¹•: {recent_danmaku or \"ï¼ˆæ— ï¼‰\"}\n",
    "æœ€é«˜ç´§æ€¥åº¦: {max_urgency:.2f}\n",
    "è¿ç»­å¿½ç•¥æ•°: {state.ignored_count}\n",
    "\n",
    "### å†³ç­–å‚è€ƒ\n",
    "urgency - cost = {max_urgency - current_cost:.2f}\n",
    "æ­£æ•°å€¾å‘å›åº”ï¼Œè´Ÿæ•°å€¾å‘ç»§ç»­\n",
    "è¿ç»­å¿½ç•¥3æ¡ä»¥ä¸Šåº”è€ƒè™‘è¡¥æ•‘\n",
    "\n",
    "## è¾“å‡ºè¦æ±‚\n",
    "è¾“å‡ºJSON:\n",
    "{{\"inner_monologue\": \"...\", \"decision\": \"continue/respond/improvise/silence\", \"speech\": \"...\", \"emotion\": \"...\"}}\n",
    "\n",
    "ç›´æ¥è¾“å‡ºJSONã€‚\"\"\"\n",
    "\n",
    "        response = self.llm.call(prompt, system=self.SYSTEM_PROMPT, max_tokens=500)\n",
    "        \n",
    "        # è§£æ\n",
    "        try:\n",
    "            if \"```\" in response:\n",
    "                json_str = response.split(\"```\")[1]\n",
    "                if json_str.startswith(\"json\"):\n",
    "                    json_str = json_str[4:]\n",
    "            else:\n",
    "                json_str = response\n",
    "            result = json.loads(json_str.strip())\n",
    "        except:\n",
    "            result = {\n",
    "                \"inner_monologue\": f\"ç»§ç»­è®²{current_node.stage}...\",\n",
    "                \"decision\": \"continue\",\n",
    "                \"speech\": f\"è¯´åˆ°{state.topic}...\",\n",
    "                \"emotion\": \"neutral\"\n",
    "            }\n",
    "        \n",
    "        # æ›´æ–°çŠ¶æ€\n",
    "        decision = result.get(\"decision\", \"continue\")\n",
    "        step_duration = 10\n",
    "        state.node_elapsed_sec += step_duration\n",
    "        state.total_elapsed_sec += step_duration\n",
    "        \n",
    "        if state.node_elapsed_sec >= current_node.duration_sec:\n",
    "            state.current_node_idx += 1\n",
    "            state.node_elapsed_sec = 0\n",
    "        \n",
    "        if decision == \"respond\" and most_urgent:\n",
    "            state.danmaku_queue = [d for d in state.danmaku_queue if d != most_urgent]\n",
    "            state.ignored_count = 0\n",
    "            result[\"target_danmaku\"] = most_urgent.text\n",
    "        elif decision == \"continue\" and state.danmaku_queue:\n",
    "            state.ignored_count += 1\n",
    "        \n",
    "        result[\"node\"] = current_node.stage\n",
    "        result[\"time\"] = f\"{state.total_elapsed_sec:.0f}s\"\n",
    "        result[\"cost\"] = current_cost\n",
    "        result[\"urgency\"] = max_urgency\n",
    "        result[\"delta\"] = max_urgency - current_cost\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_ending(self, state: PerformanceState) -> Dict:\n",
    "        return {\n",
    "            \"inner_monologue\": \"è¯¥æ”¶å°¾äº†~\",\n",
    "            \"decision\": \"continue\",\n",
    "            \"speech\": f\"å¥½å•¦ï¼Œä»Šå¤©å…³äº{state.topic}å°±èŠåˆ°è¿™é‡Œï¼Œä¸‹æ¬¡è§ï¼\",\n",
    "            \"emotion\": \"satisfied\",\n",
    "            \"node\": \"END\",\n",
    "            \"time\": f\"{state.total_elapsed_sec:.0f}s\"\n",
    "        }\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "performer = CognitivePerformer(llm, analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: å®Œæ•´Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchuuEngine:\n",
    "    \"\"\"echuuæ ¸å¿ƒå¼•æ“\"\"\"\n",
    "    \n",
    "    def __init__(self, llm: LLMClient, analyzer: PatternAnalyzer):\n",
    "        self.llm = llm\n",
    "        self.analyzer = analyzer\n",
    "        self.script_gen = ScriptGenerator(llm, analyzer)\n",
    "        self.performer = CognitivePerformer(llm, analyzer)\n",
    "    \n",
    "    def create_performance(self, name: str, persona: str, background: str,\n",
    "                          topic: str, language: str = \"zh\") -> PerformanceState:\n",
    "        \"\"\"åˆ›å»ºè¡¨æ¼”\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ¬ æ­£åœ¨ç”Ÿæˆå‰§æœ¬...\")\n",
    "        script = self.script_gen.generate(name, persona, background, topic, language)\n",
    "        \n",
    "        # æå–å‚è€ƒæ•°æ®\n",
    "        catchphrases = [cp for cp, _ in self.analyzer.extract_catchphrases(language)[:5]]\n",
    "        hooks = self.analyzer.extract_hooks(language)[:3]\n",
    "        punchlines = self.analyzer.extract_punchlines(language)[:3]\n",
    "        \n",
    "        return PerformanceState(\n",
    "            name=name,\n",
    "            persona=persona,\n",
    "            background=background,\n",
    "            topic=topic,\n",
    "            script=script,\n",
    "            catchphrases=catchphrases,\n",
    "            example_hooks=hooks,\n",
    "            example_punchlines=punchlines\n",
    "        )\n",
    "    \n",
    "    def run(self, state: PerformanceState, danmaku_sim: List[Dict] = None,\n",
    "            max_steps: int = 12) -> List[Dict]:\n",
    "        \"\"\"è¿è¡Œè¡¨æ¼”\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        danmaku_by_step = defaultdict(list)\n",
    "        if danmaku_sim:\n",
    "            for d in danmaku_sim:\n",
    "                step = d.get(\"step\", 0)\n",
    "                danmaku_by_step[step].append(Danmaku.from_text(d.get(\"text\", \"\")))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ­ {state.name} - {state.topic}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ å‰§æœ¬éª¨æ¶:\")\n",
    "        for i, node in enumerate(state.script):\n",
    "            cost_bar = \"â–ˆ\" * int(node.interruption_cost * 5) + \"â–‘\" * (5 - int(node.interruption_cost * 5))\n",
    "            print(f\"  [{i+1}] {node.stage} {cost_bar} cost={node.interruption_cost:.1f}\")\n",
    "            print(f\"      â†’ {node.goal}\")\n",
    "        \n",
    "        print(f\"\\nğŸ¬ å¼€å§‹è¡¨æ¼”...\\n\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            new_danmaku = danmaku_by_step.get(step, [])\n",
    "            result = self.performer.step(state, new_danmaku)\n",
    "            results.append(result)\n",
    "            \n",
    "            # ç¾åŒ–è¾“å‡º\n",
    "            time_str = result.get('time', '?')\n",
    "            node_str = result.get('node', '?')\n",
    "            decision = result.get('decision', 'continue').upper()\n",
    "            \n",
    "            # å†³ç­–æŒ‡ç¤ºå™¨\n",
    "            if decision == \"RESPOND\":\n",
    "                dec_icon = \"ğŸ’¬\"\n",
    "            elif decision == \"CONTINUE\":\n",
    "                dec_icon = \"ğŸ“–\"\n",
    "            elif decision == \"SILENCE\":\n",
    "                dec_icon = \"ğŸ¤«\"\n",
    "            else:\n",
    "                dec_icon = \"ğŸ­\"\n",
    "            \n",
    "            print(f\"[{time_str}] {node_str} {dec_icon}\")\n",
    "            print(f\"  ğŸ’­ {result.get('inner_monologue', '')}\")\n",
    "            print(f\"  ğŸ“¢ {result.get('speech', '(æ²‰é»˜)')}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºå†³ç­–ç»†èŠ‚\n",
    "            if 'delta' in result:\n",
    "                delta = result['delta']\n",
    "                if delta > 0:\n",
    "                    delta_str = f\"â†‘{delta:.2f}\"\n",
    "                else:\n",
    "                    delta_str = f\"â†“{abs(delta):.2f}\"\n",
    "                print(f\"  ğŸ“Š urgency={result.get('urgency', 0):.2f} - cost={result.get('cost', 0):.2f} = {delta_str}\")\n",
    "            \n",
    "            if result.get('target_danmaku'):\n",
    "                print(f\"  ğŸ’¬ å›åº”: {result['target_danmaku']}\")\n",
    "            print()\n",
    "            \n",
    "            if result.get('node') == \"END\":\n",
    "                break\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"âœ… è¡¨æ¼”ç»“æŸï¼æ€»æ—¶é•¿: {state.total_elapsed_sec:.0f}ç§’\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def export_script(self, state: PerformanceState) -> Dict:\n",
    "        \"\"\"å¯¼å‡ºå‰§æœ¬ä¸ºJSON\"\"\"\n",
    "        return {\n",
    "            \"character\": {\n",
    "                \"name\": state.name,\n",
    "                \"persona\": state.persona,\n",
    "                \"background\": state.background\n",
    "            },\n",
    "            \"topic\": state.topic,\n",
    "            \"script\": [\n",
    "                {\n",
    "                    \"stage\": n.stage,\n",
    "                    \"goal\": n.goal,\n",
    "                    \"attention\": n.target_attention,\n",
    "                    \"speech_act\": n.target_speech_act,\n",
    "                    \"duration_sec\": n.duration_sec,\n",
    "                    \"interruption_cost\": n.interruption_cost,\n",
    "                    \"hint\": n.content_hint\n",
    "                }\n",
    "                for n in state.script\n",
    "            ],\n",
    "            \"catchphrases\": state.catchphrases\n",
    "        }\n",
    "\n",
    "\n",
    "# åˆå§‹åŒ–Engine\n",
    "engine = EchuuEngine(llm, analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: æµ‹è¯•è¿è¡Œï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ç”¨ä¾‹1: åŸºäºçœŸå®åˆ‡ç‰‡æ•°æ®çš„è¯é¢˜\n",
    "test_case = {\n",
    "    \"name\": \"å…­èº\",\n",
    "    \"persona\": \"25å²ä¸»æ’­ï¼Œæ´»æ³¼è‡ªå˜²ï¼Œå–œæ¬¢åˆ†äº«ç”Ÿæ´»ç»å†ï¼Œå£ç™–ï¼šæˆ‘è§‰å¾—ã€å¯¹å§\",\n",
    "    \"background\": \"åšè¿‡å¾ˆå¤šå·¥ä½œï¼Œç°åœ¨æ˜¯å…¨èŒä¸»æ’­ï¼Œç•™å­¦æ—¥æœ¬å¤šå¹´\",\n",
    "    \"topic\": \"ç•™å­¦æ—¶å·åƒå®¤å‹è…°æœçš„æ•…äº‹\",\n",
    "    \"danmaku\": [\n",
    "        {\"step\": 1, \"text\": \"å“ˆå“ˆå“ˆ\"},\n",
    "        {\"step\": 3, \"text\": \"æˆ‘ä¹Ÿæœ‰ç±»ä¼¼ç»å†\"},\n",
    "        {\"step\": 5, \"text\": \"[SC Â¥50] åæ¥å®¤å‹ç”Ÿæ°”äº†å—\"},\n",
    "        {\"step\": 7, \"text\": \"ç¬‘æ­»\"},\n",
    "        {\"step\": 9, \"text\": \"å¤ªçœŸå®äº†\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºè¡¨æ¼”\n",
    "state = engine.create_performance(\n",
    "    name=test_case[\"name\"],\n",
    "    persona=test_case[\"persona\"],\n",
    "    background=test_case[\"background\"],\n",
    "    topic=test_case[\"topic\"],\n",
    "    language=\"zh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å‡ºé¢„ç½®å‰§æœ¬\n",
    "script_json = engine.export_script(state)\n",
    "print(\"\\nğŸ“„ é¢„ç½®å‰§æœ¬JSON:\")\n",
    "print(json.dumps(script_json, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œè¡¨æ¼”ï¼ˆå¸¦æ¨¡æ‹Ÿå¼¹å¹•ï¼‰\n",
    "results = engine.run(state, test_case[\"danmaku\"], max_steps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å‡ºè¡¨æ¼”æ—¥å¿—\n",
    "print(\"\\nğŸ“„ è¡¨æ¼”æ—¥å¿—:\")\n",
    "for r in results:\n",
    "    print(f\"[{r.get('time')}] {r.get('node')} | {r.get('decision')} | ğŸ’­ {r.get('inner_monologue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Interruption Cost å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–å†³ç­–è¿‡ç¨‹\n",
    "print(\"\\nğŸ“Š å†³ç­–è¿‡ç¨‹å¯è§†åŒ–:\")\n",
    "print(\"\\næ—¶é—´ | é˜¶æ®µ | urgency | cost | Î” | å†³ç­–\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for r in results:\n",
    "    if r.get('node') == 'END':\n",
    "        continue\n",
    "    \n",
    "    time = r.get('time', '?')\n",
    "    node = r.get('node', '?')[:10].ljust(10)\n",
    "    urgency = r.get('urgency', 0)\n",
    "    cost = r.get('cost', 0)\n",
    "    delta = r.get('delta', urgency - cost)\n",
    "    decision = r.get('decision', 'continue')\n",
    "    \n",
    "    # å¯è§†åŒ–bar\n",
    "    urg_bar = \"â–ˆ\" * int(urgency * 5) + \"â–‘\" * (5 - int(urgency * 5))\n",
    "    cost_bar = \"â–ˆ\" * int(cost * 5) + \"â–‘\" * (5 - int(cost * 5))\n",
    "    \n",
    "    dec_icon = \"ğŸ’¬\" if decision == \"respond\" else \"ğŸ“–\"\n",
    "    \n",
    "    print(f\"{time:>5} | {node} | {urg_bar} {urgency:.1f} | {cost_bar} {cost:.1f} | {delta:+.2f} | {dec_icon} {decision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: æµ‹è¯•ç”¨ä¾‹2 - ä¸åŒè¯é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ç”¨ä¾‹2: æƒ…æ„Ÿç±»è¯é¢˜ï¼ˆé«˜costï¼‰\n",
    "test_case_2 = {\n",
    "    \"name\": \"Cathy\",\n",
    "    \"persona\": \"A gentle yet alert catgirl observer, playful on the surface, quietly perceptive.\",\n",
    "    \"background\": \"Born at the edge of data and dreams, she learned language through silent companionship.\",\n",
    "    \"topic\": \"è¢«æœ‹å‹è¯¯è§£çš„é‚£æ¬¡ç»å†\",\n",
    "    \"danmaku\": [\n",
    "        {\"step\": 2, \"text\": \"å¿ƒç–¼\"},\n",
    "        {\"step\": 4, \"text\": \"[SC Â¥100] åæ¥å’Œå¥½äº†å—\"},  # é«˜urgency SC\n",
    "        {\"step\": 6, \"text\": \"æˆ‘ä¹Ÿæœ‰è¿‡ç±»ä¼¼ç»å†\"},\n",
    "        {\"step\": 8, \"text\": \"ç ´é˜²äº†\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "state_2 = engine.create_performance(\n",
    "    name=test_case_2[\"name\"],\n",
    "    persona=test_case_2[\"persona\"],\n",
    "    background=test_case_2[\"background\"],\n",
    "    topic=test_case_2[\"topic\"],\n",
    "    language=\"zh\"\n",
    ")\n",
    "\n",
    "results_2 = engine.run(state_2, test_case_2[\"danmaku\"], max_steps=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "### æ ¸å¿ƒæœºåˆ¶å›é¡¾\n",
    "\n",
    "1. **Interruption Cost** - ä»çœŸå®æ•°æ®å­¦ä¹ åˆ°çš„\"è¢«æ‰“æ–­ä»£ä»·\"\n",
    "   - `self` æ³¨æ„åŠ›æ—¶ cost è¾ƒé«˜ï¼ˆä¸æƒ³è¢«æ‰“æ–­è®²è‡ªå·±çš„äº‹ï¼‰\n",
    "   - `audience` æ³¨æ„åŠ›æ—¶ cost è¾ƒä½ï¼ˆæœ¬æ¥å°±åœ¨äº’åŠ¨ï¼‰\n",
    "   - cost éšèŠ‚ç‚¹è¿›åº¦è¡°å‡ï¼ˆå¿«è®²å®Œæ—¶å¯ä»¥è½¬åœºï¼‰\n",
    "\n",
    "2. **å†³ç­–å…¬å¼**\n",
    "   ```\n",
    "   decision_value = urgency - cost\n",
    "   æ­£æ•° â†’ å›åº”å¼¹å¹•\n",
    "   è´Ÿæ•° â†’ ç»§ç»­å‰§æœ¬\n",
    "   ```\n",
    "\n",
    "3. **Inner Monologue** - Killer Feature\n",
    "   - è®©è§‚ä¼—çœ‹åˆ°AIçš„\"æ€è€ƒè¿‡ç¨‹\"\n",
    "   - \"é€‰æ‹©ä¸å›åº”\"ä¹Ÿæ˜¯ä¸€ç§è¡¨è¾¾\n",
    "   - è¿™æ˜¯echuuåŒºåˆ«äºORIBAçš„æ ¸å¿ƒ\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. æ¥å…¥çœŸå®LLMç”Ÿæˆæ›´ä¸°å¯Œçš„å†…å®¹\n",
    "2. æ·»åŠ TTS/Live2Dé›†æˆ\n",
    "3. å®ç°çœŸå®å¼¹å¹•æµå¤„ç†\n",
    "4. ä¼˜åŒ–å£ç™–å’Œè¯­è¨€é£æ ¼çš„ä¸ªæ€§åŒ–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
